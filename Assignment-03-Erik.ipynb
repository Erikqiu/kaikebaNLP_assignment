{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import math\n",
    "from collections import Counter,defaultdict\n",
    "from icecream import ic\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assuming_function(x):\n",
    "    return 6.18*x+ 8 + random.randint(-3,3)\n",
    "\n",
    "random_data = np.random.random((50,2))\n",
    "X = random_data[:,0]\n",
    "y = [assuming_function(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,y)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X.reshape(-1,1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46392031469739475"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X.reshape(-1,1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.25698886])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.81825197741081"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_function(x):\n",
    "    return lr.coef_*x + lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,y)\n",
    "plt.plot(X,fit_function(X),color='r')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.82384306])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict([[new_data]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def knn_model(X, y):\n",
    "    \n",
    "    return [(X_i,y_i) for X_i, y_i in zip(X,y)]\n",
    "\n",
    "def cosine_distance(x1, x2):\n",
    "    return cosine(x1, x2)\n",
    "\n",
    "def knn_predict(model,x, k=5):\n",
    "    most_similars = sorted(model,key=lambda x_i:cosine_distance(x_i[0],x))[:k]\n",
    "    \n",
    "    y_hats = [_y for x,_y in most_similars]\n",
    "    \n",
    "    print(most_similars)\n",
    "    return np.mean(y_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8181319490861809, 16.056055445352598), (0.08947119760480227, 8.552932001197679), (0.4120540478374103, 13.546494015635195), (0.9268878345347222, 10.728166817424583), (0.41563378863022316, 7.56861681373478)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.290453018668966"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_predict(knn_model(X,y),0.8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Entropy = - \\sum^{n}_{i=1} Pr(x_i)log(Pr(x_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "\n",
    "def entropy(elements):\n",
    "    counter = Counter(elements)\n",
    "    probs = [counter[c] / len(elements) for c in set(elements)]\n",
    "    ic(probs)\n",
    "    return - sum(p * np.log(p) for p in probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "   # 'pet': [1, 1, 1, 0, 0, 0, 1],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "df_bought = pd.DataFrame(mock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_optimal_spilter(training_data: pd.DataFrame, target: str) -> str:\n",
    "    x_fields = set(training_data.columns.tolist()) - {target}\n",
    "    spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    \n",
    "    for f in x_fields:\n",
    "        ic(f)\n",
    "        values = set(training_data[f])\n",
    "        ic(values)\n",
    "        for v in values:\n",
    "            sub_spliter_1 = training_data[training_data[f] == v][target].tolist()\n",
    "            ic(sub_spliter_1)\n",
    "            # split by the current feature and one value\n",
    "            \n",
    "            entropy_1 = entropy(sub_spliter_1)\n",
    "            ic(entropy_1)\n",
    "            \n",
    "            sub_spliter_2 = training_data[training_data[f] != v][target].tolist()\n",
    "            ic(sub_spliter_2)\n",
    "            \n",
    "            entropy_2 = entropy(sub_spliter_2)\n",
    "            ic(entropy_2)\n",
    "            \n",
    "            entropy_v = entropy_1 + entropy_2\n",
    "            ic(entropy_v)\n",
    "            \n",
    "            if entropy_v <= min_entropy:\n",
    "                min_entropy = entropy_v\n",
    "                spliter = (f, v)\n",
    "    \n",
    "    print('spliter is: {}'.format(spliter))\n",
    "    print('the min entropy is: {}'.format(min_entropy))\n",
    "    \n",
    "    return spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_2: 0.6730116670092565\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| sub_spliter_1: [1, 1, 1, 0]\n",
      "ic| probs: [0.25, 0.75]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_spliter_2: [0, 0, 1]\n",
      "ic| probs: [0.6666666666666666, 0.3333333333333333]\n",
      "ic| entropy_2: 0.6365141682948128\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| sub_spliter_1: [0, 0, 1]\n",
      "ic| probs: [0.6666666666666666, 0.3333333333333333]\n",
      "ic| entropy_1: 0.6365141682948128\n",
      "ic| sub_spliter_2: [1, 1, 1, 0]\n",
      "ic| probs: [0.25, 0.75]\n",
      "ic| entropy_2: 0.5623351446188083\n",
      "ic| entropy_v: 1.198849312913621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('family_number', 2)\n",
      "the min entropy is: 0.6730116670092565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('family_number', 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_optimal_spilter(training_data=df_bought, target='bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "2      F    +10              2       1\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bought[df_bought['family_number'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bought[df_bought['family_number'] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| sub_spliter_1: [1, 0, 0, 0]\n",
      "ic| probs: [0.75, 0.25]\n",
      "ic| entropy_1: 0.5623351446188083\n",
      "ic| sub_spliter_2: [1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| sub_spliter_1: [1]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 0, 0, 0]\n",
      "ic| probs: [0.75, 0.25]\n",
      "ic| entropy_2: 0.5623351446188083\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| probs: [0.6, 0.4]\n",
      "ic| entropy_1: 0.6730116670092565\n",
      "ic| sub_spliter_2: []\n",
      "ic| probs: []\n",
      "ic| entropy_2: 0\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| sub_spliter_1: [1, 1, 0]\n",
      "ic| probs: [0.3333333333333333, 0.6666666666666666]\n",
      "ic| entropy_1: 0.6365141682948128\n",
      "ic| sub_spliter_2: [0, 0]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| sub_spliter_1: [0, 0]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 1, 0]\n",
      "ic| probs: [0.3333333333333333, 0.6666666666666666]\n",
      "ic| entropy_2: 0.6365141682948128\n",
      "ic| entropy_v: 0.6365141682948128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('income', '-10')\n",
      "the min entropy is: 0.5623351446188083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('income', '-10')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_optimal_spilter(df_bought[df_bought['family_number'] != 2], 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "1      F    -10              1       1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_n_1 = df_bought[df_bought['family_number'] != 2]\n",
    "fm_n_1[fm_n_1['income'] == '-10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_n_1[fm_n_1['income'] != '-10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| sub_spliter_1: [1, 0]\n",
      "ic| probs: [0.5, 0.5]\n",
      "ic| entropy_1: 0.6931471805599453\n",
      "ic| sub_spliter_2: [0, 0]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6931471805599453\n",
      "ic| sub_spliter_1: [0, 0]\n",
      "ic| probs: [1.0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 0]\n",
      "ic| probs: [0.5, 0.5]\n",
      "ic| entropy_2: 0.6931471805599453\n",
      "ic| entropy_v: 0.6931471805599453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('gender', 'M')\n",
      "the min entropy is: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('gender', 'M')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_optimal_spilter(fm_n_1.loc[fm_n_1['income'] != '-10',['gender','bought']], 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  bought\n",
       "0      F       1\n",
       "3      F       0\n",
       "4      M       0\n",
       "5      M       0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_n_2 = fm_n_1.loc[fm_n_1['income'] != '-10',['gender','bought']]\n",
    "fm_n_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = [random.randint(0, 150) for _ in range(100)]\n",
    "X2 = [random.randint(0, 150) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1714f80f320>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X1, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tranning_data = [[x1, x2] for x1, x2 in zip(X1, X2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=6, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=500,\n",
       "       n_clusters=6, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.fit(tranning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18.09090909,  88.36363636],\n",
       "       [ 94.76470588,  62.52941176],\n",
       "       [ 62.79166667, 129.16666667],\n",
       "       [118.22222222,  21.05555556],\n",
       "       [ 25.28571429,  31.42857143],\n",
       "       [127.0625    , 116.1875    ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 3, 2, 3, 2, 0, 0, 5, 1, 5, 2, 5, 2, 2, 1, 0, 1, 0, 5, 0, 0,\n",
       "       1, 3, 1, 2, 4, 0, 4, 2, 3, 2, 1, 3, 3, 5, 5, 4, 1, 2, 0, 3, 5, 2,\n",
       "       1, 3, 4, 4, 5, 1, 2, 2, 1, 2, 4, 3, 3, 2, 2, 2, 2, 3, 2, 3, 4, 1,\n",
       "       1, 4, 4, 1, 0, 2, 5, 5, 5, 3, 4, 1, 2, 3, 0, 5, 3, 2, 2, 5, 2, 1,\n",
       "       3, 5, 1, 4, 5, 4, 3, 2, 5, 0, 1, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = defaultdict(list)\n",
    "for label, location in zip(cluster.labels_, tranning_data):\n",
    "    centers[label].append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "color = ['red', 'green', 'grey', 'black', 'yellow', 'orange']\n",
    "\n",
    "for i, c in enumerate(centers):\n",
    "    for location in centers[c]:\n",
    "        plt.scatter(*location, c=color[i])\n",
    "        \n",
    "for center in cluster.cluster_centers_:\n",
    "    plt.scatter(*center, s=100,marker='x',c='b')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 模型是对事物的合理简化而抽象出来的物件，因为影响模型的因素非常多，所以模型很可能都是错的，但是在特定的条件下的模型是对的是有用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "过拟合：算法所训练的模型过多地表达了数据之间的噪音关系，模型泛化能力过强，存在较大的方差\n",
    "\n",
    "过拟合产生的原因：\n",
    "* 样本量过少\n",
    "* 样本噪音过大\n",
    "* 模型过于复杂\n",
    "\n",
    "\n",
    "欠拟合：算法所训练的模型不能很好地表达了数据之间的关系，模型泛化能力过弱，存在较大的偏差\n",
    "\n",
    "欠过拟合产生的原因：\n",
    "* 特征过少\n",
    "* 样本噪音过大\n",
    "* 模型复杂度过低"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "\n",
    "precision 是指预测为“是”并且预测正确的个数/预测为“是”的个数\n",
    "$$ precision = \\frac{TP}{TP + NP} $$\n",
    "\n",
    "recall 是指预测为“是”并且预测正确的个数/实际上为“是”的个数\n",
    "$$ precision = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "ROC/AUC ROC: receiver oprating characteristic curve 由TPR和FPR组成的曲线\n",
    "\n",
    "AUC: area under curve ROC曲线下的面积，ROC越大表现越好\n",
    "$$TPR = \\frac{TP}{TP + FN} $$\n",
    "$$FPR = \\frac{FP}{FP + TN} $$\n",
    "\n",
    "F1、F2 由于precision和recall是此消彼长的，我们有时候需要平衡这两个标准，于是引入了F-score\n",
    "$$ F-score = (1+\\beta^2)\\cdot \\frac{precision \\cdot recall}{\\beta^2 \\cdot precision + recall} $$\n",
    "\n",
    "$$ \\beta = 1 时为F1，\\beta = 2 时为F2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:传统的分析式编程的思维是：通过条件判断来得出最终的结果\n",
    "\n",
    "机器学习的思维是：通过输入数据，让模型来学习数据中存在的某种模式，来计算出最终得出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 这句话正确，因为机器学习模型的评价标准其实有很多，但是并不都适合于我们所输入的数据或者我们想要达到的目标，所以正确定义了一个可以衡量我们的目标的评价标准就已经解决了一半"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "\n",
    "def entropy(elements):\n",
    "    counter = Counter(elements)\n",
    "    probs = [counter[c] / len(elements) for c in set(elements)]\n",
    "#     ic(probs)\n",
    "    return - sum(p * np.log(p) for p in probs)\n",
    "\n",
    "def find_the_optimal_spilter(training_data: pd.DataFrame, target: str) -> str:\n",
    "    x_fields = set(training_data.columns.tolist()) - {target}\n",
    "    \n",
    "    spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    \n",
    "    for f in x_fields:\n",
    "#         ic(f)\n",
    "        values = set(training_data[f])\n",
    "#         ic(values)\n",
    "        for v in values:\n",
    "            sub_spliter_1 = training_data[training_data[f] == v][target].tolist()\n",
    "#             ic(sub_spliter_1)\n",
    "            # split by the current feature and one value\n",
    "            \n",
    "            entropy_1 = entropy(sub_spliter_1)\n",
    "#             ic(entropy_1)\n",
    "            \n",
    "            sub_spliter_2 = training_data[training_data[f] != v][target].tolist()\n",
    "#             ic(sub_spliter_2)\n",
    "            \n",
    "            entropy_2 = entropy(sub_spliter_2)\n",
    "#             ic(entropy_2)\n",
    "            \n",
    "            entropy_v = entropy_1 + entropy_2\n",
    "#             ic(entropy_v)\n",
    "            \n",
    "            if entropy_v <= min_entropy:\n",
    "                min_entropy = entropy_v\n",
    "                spliter = (f, v)\n",
    "    \n",
    "#     print('spliter is: {}'.format(spliter))\n",
    "#     print('the min entropy is: {}'.format(min_entropy))\n",
    "    \n",
    "    return spliter\n",
    "\n",
    "\n",
    "def get_spilt(training_data,target,feature,threshold):\n",
    "    a = (training_data[feature]==threshold)\n",
    "    return training_data[a].drop(columns=feature)\n",
    "\n",
    "    \n",
    "def decision_tree_model(training_data: pd.DataFrame, target: str):\n",
    "    lst_target_value = training_data[target].value_counts()\n",
    "#     ic(lst_target_value)\n",
    "    if lst_target_value.iloc[0] == training_data[target].shape[0]:\n",
    "        return 1\n",
    "    if len(training_data.columns.tolist())==1:\n",
    "        return lst_target_value.loc[1]/training_data[target].shape[0]\n",
    "#     lst_feature = set(training_data.columns.tolist()) - {target}\n",
    "\n",
    "    best_f,best_t= find_the_optimal_spilter(training_data,target)\n",
    "    my_tree = {best_f:{}}\n",
    "#     ic(best_f)\n",
    "    for v_i in training_data[best_f].unique():\n",
    "#         ic(v_i)\n",
    "        subtree= get_spilt(training_data,target,best_f,v_i)\n",
    "        my_tree[best_f][v_i] = decision_tree_model(subtree,target)\n",
    "    return my_tree\n",
    "\n",
    "\n",
    "def get_predict(model_tree,test_data):\n",
    "#     output = test_data.copy()\n",
    "    first_str = list(model_tree.keys())[0]\n",
    "    second_dic = model_tree[first_str]\n",
    "\n",
    "    v_i = test_data[first_str]\n",
    "#     ic(v_i)\n",
    "    for key_i in second_dic.keys():\n",
    "#         ic(key_i)\n",
    "        if v_i == key_i:\n",
    "            if isinstance(second_dic[v_i],dict):\n",
    "                pred = get_predict(second_dic[v_i],test_data)\n",
    "            else:\n",
    "                pred = second_dic[v_i]\n",
    "    return pred\n",
    "\n",
    "def decision_tree_predict(model_tree,test_data):\n",
    "    output = test_data.copy()\n",
    "    for index_i,row_i in output.iterrows():\n",
    "        pred = get_predict(model_tree,output.loc[index_i,:])\n",
    "        output.loc[index_i,'prediction'] = pred\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "   # 'pet': [1, 1, 1, 0, 0, 0, 1],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "df_bought = pd.DataFrame(mock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = decision_tree_model(df_bought,'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number\n",
       "0      M    +10              1\n",
       "1      F    -10              2\n",
       "2      F    +10              1"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bought_test = pd.DataFrame({\"gender\":['M','F','F'],\n",
    "                               \"income\":['+10','-10','+10'],\n",
    "                               \"family_number\":[1,2,1]}) \n",
    "df_bought_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bought_test.loc[0,:]['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  prediction\n",
       "0      M    +10              1         1.0\n",
       "1      F    -10              2         1.0\n",
       "2      F    +10              1         0.5"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree_predict(dt_model,df_bought_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "from sklearn.datasets import load_boston\n",
    "import random\n",
    "dataset = load_boston()\n",
    "x,y=dataset['data'],dataset['target']\n",
    "X_rm = x[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2678d47c588>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD4CAYAAAAaT9YAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2df5AcZ5nfv8+O2tasCR7JLMQeW8hcKAl0Qlq8hZVTFXUSF+vA2Gz8E8dQVIqK8weVYHDtIVIEy1dOLKIQ+/5IkXJB7nxlzsiWYbFxgrjCulzFVTYneS18iq3KgW2ZkYIF1hqQxvLs7pM/Zno0M9tv99s9/Xu+nyrVaudH99M9O99++3m+7/OKqoIQQkgxGcs6AEIIIdGhiBNCSIGhiBNCSIGhiBNCSIGhiBNCSIFZkebO3vGOd+jatWvT3CUhhBSeQ4cO/UpVJ7yeS1XE165di4MHD6a5S0IIKTwi8orpOaZTCCGkwFDECSGkwFDECSGkwFDECSGkwFDECSGkwFi5U0TkZQC/BbAIYEFVp0RkNYC9ANYCeBnATap6KpkwybDMzjWwZ/9RHJ9v4pJaFTM71mF6sp51WEMR9zGleY7i3Ffan21c+3O305hvoiKCRdXuz3FnDM2FJagCFRHccuVluHt649D7H3zvtvUTOPDiyVg/h97jqafweYhNF8OOiE+p6q96HvtPAF5X1d0ishPAKlX9kt92pqamlBbD9Jmda+DL330ezdZi97GqU8E9120srJDHfUxpnqM495X2ZxvX/ry2E8SntqzB1LtXR96/zT7j/ByG3WYvInJIVae8nhsmnfIJAA90/v8AgOkhtkUSZM/+o8v+uJqtRezZfzSjiIYn7mNK8xzFua+0P9u49ue1nSAeeubVofZvs884P4dht2mLrYgrgB+JyCERua3z2LtU9QQAdH6+0+uNInKbiBwUkYMnT54cPmISmuPzzVCPF4G4jynNcxTnvtL+bOPaX5T4FlWH2r/tPuP8HIbZpi22Ir5VVT8I4KMAPiciH7bdgarer6pTqjo1MeE5a5QkzCW1aqjHi0Dcx5TmOYpzX2l/tnHtL0p8FZGh9m+7zzg/h2G2aYuViKvq8c7P1wB8D8CHAPxSRC4GgM7P15IKkgzHzI51qDqVvseqTgUzO9ZlFNHwxH1MaZ6jOPeV9mcb1/68thPELVdeNtT+bfYZ5+cw7DZtCXSniMgFAMZU9bed/18F4E8BPAbgMwB2d35+P7EoyVC4BZUyuVPiPqY0z1Gc+0r7s41rf73bCetOibp/r9hdd4obQ2/+OswxmY4nF+4UEXkP2qNvoC36f6Wq/0FELgLwMIA1AI4BuFFVX/fbFt0phBAvsrTAFsG95edOCRyJq+rPAWzyePzXAD4yfHiEkFFmUEQb8018+bvPAwg3Gvbbvt8Fws/xkhcR94MzNgkhmZKkTdK9QDTmm1Ccu0DMzjW6rym6e4siTgjJlCRF1OYCUXT3FkWcEJIpSYqozQWi6O4tijghJFOSFFGbC8T0ZB33XLcR9VoVAqBeq+aqqBlEqsuzEULIIEnaJGd2rPN0ngxeIKYn64UR7UEo4oSQzElKRMs4R2IQijghJDHy0AK5yKNsGyjihJBESNr/TdqwsEkISYQytkDOIxRxQkjszM410Cj4JJqiQBEnhMSKm0YxUZRJNEWBOXFCSKz4rXIzaO/LQ+Gz6FDECSGx4pcu6Z1Ew8JnPDCdQgiJFVO6pF6rWncPJPZQxAkhsWI7jT7N7oGzcw1s3f0kLt/5BLbufrKvi2HRYTqFEBIrtrMkL6lVPR0scRc+y562oYgTQmLHZpakbV+TYSn6og9BUMQJIZmQVl+Toi/6EARFnBBiRRJ2wDT6mqSVtskKFjYJIYHYLHMWdbtJFxyLvuhDEBRxQkggprzyrseORN5mUheGQYq+6EMQTKcQQgIx5Y/nmy3MzjUiCWKaBccyt6PlSJwQEohf/vj2vc9FSoWUveCYFhRxQkggQfnjKKmQNFeZL/NkH4o4ISSQ6ck6Vo07vq8JO2U+rYJjWrn3rKCIE0KsuPOaDctEd5DGfNN6xJtWwbHsPVpY2CSEWNE7Oce04IMA3edsprenUXAse+6dI3FCiDXTk3U8tXM77rt587JRuQDQgdfnYcSbZu49CyjihJDQeKVCBgXcJesRr1fu3akITp9dKEWhk+kUQgpGXlbDGUyFbN39ZC6ntw/2aKmNO/jdmwuYb7YAFL+rIUfihBSIPDst8jy93U0DvbT7aoyftwKtpf77hjykfaJCESekQOTZaVGU6e1lK3QynUJIgci7ABVhenvZuhpyJE5IgSi70yIN8pz2iQJFnJACUTYByoKipH1ssU6niEgFwEEADVX9uIhcDuA7AFYDeBbAp1X1rWTCJIQA6a2GU3aKkPaxJUxO/PMAXgDw9s7vXwNwr6p+R0T+G4DPAvhGzPERQgYougDlxSJZFqzSKSJyKYCrAXyz87sA2A5gX+clDwCYTiJAQkh5yLNFsqjY5sTvA/AnAJY6v18EYF5VFzq//wKA56VURG4TkYMicvDkyZNDBUsIKTZ5tkgWlUARF5GPA3hNVQ/1PuzxUs9Zt6p6v6pOqerUxMRExDAJIWUg7xbJImKTE98K4FoR+RiAlWjnxO8DUBORFZ3R+KUAjicXJiGkDJTNo50HAkfiqvplVb1UVdcC+CSAJ1X1VgAHANzQedlnAHw/sSgJIaUgTotkmVfrCcMwPvEvAfiiiPwD2jnyb8UTEiGkrMTl0WaB9ByiamogGT9TU1N68ODB1PZHCCknpo6J9VoVT+3cnkFEySIih1R1yus59k4hpOSU0ZfNAuk5OO2ekBJT1rQDe8icgyJOSIkpoy97dq6BM28tLHt8VHvIMJ1CSEGxSZOULe3g3lkMXphqVQe7rt1Q+DRRFCjihBSQr8w+j28/faw7w860xNiFVae7DFkvF1adNMKMHa87CwC44PwVIyngANMphBSO2blGn4C7eKVJxGtutc/jeadsdxZxQBEnpGDs2X/UemX5+TPLR+F+j+cdFjSXQxEnpGD4jToHxcwkbgpg8k9/VDiXChfFWA5FnIwsRZ22bRJmAZaJ2cyOdXAq3rmTU2dauH3vc4US87KtyhMHLGySkWTQ5WAqDOaRmR3rljk0BMCtW9Z4xx4wKfvUmVb32IH8rxpU9EUx4oYiTkYSP/903gUizBJte/YfRWspuLVGs7WIux4/gjdbS4W8sI0yTKeQkaToLofpyTpmdqzDJbUqjs83sWf/Uc+USJjjOXWmVbqJQaMAR+JkJCl6X2vbdJDpOMMw7IWtjL1b8gRH4mQkybvLIajoajud3us4wzLMha2svVvyBEWcjCR5djnYCJ9pdNyYb/aJfu9xRmHYC1sZe7fkDaZTyMiSV5eDTdG1Nu7glGHCzmBqZXqyjoOvvI4Hnz4WuO+qM4bVF5wfW+qj6LWHIkARJyRn2Ahf0Foug6L/0DOvWu17YUljzVkXvfZQBJhOISRn2Ewtf8OjqdUgvaK/aLmCV2tRY0115L32UAYo4oTkDBvhsxnJ9r6mEqLjVZypjjzXHsoC0ymE5AybyTxeszZ7GRT9W668zConDsSf6shr7aEsUMQJySFBwuc+d9fjR7oFTkF7hn3dQ/Tvnt6I7z3bwOm3vEXfxSvVQZ93vqGIE5JDbITz4Cuv97WUVZwTYS+RPRMg4Oe20h/HzL7DaC22H2/MNzGz7zAATsXPCxRxQnKGzWxMm4Uh9uw/isZ8ExURLKp2f/rRbC1h5pFzIn3X40e6Au7SWlTc9fgRinhOYGGTkJxhM0HGb2EIV/Rda58r3NYOlaVzDhWTF930OEkfjsQJSYiouWQbn7ifg6QiYix4Audy51FiIPmDIk5IAoTtV94r+GOGtEeva8Q0iUYQPOK2GY+7+6oZFlquFXSh5TLCdAohCRCmZ8hgrxSTCJ8+u9DtieLlJXcXhhhWYJ0x6TpUdl27Ac6YLHt+17UbhtoHiQ+OxAlJgDA9Q7wE34v5ZmvZaN4rXfPET08Yt1F1KljpjBlz2rWqg13XbuhuP8wCFCQbKOKEJECYniFh8s+9PVFMXnK/leybrUWcv2IMTkX6XCdVp+I5k5Ie8fxDESckAbatn1hmATT1DAm7cEOQ6Adtb77ZgjMmWDXuYP5Ma5k4u8LdmG/2FUG5XFs+YU6ckJiZnWvg0UONPgEXANdf4T1yDrtww4UBOW+b7bWWFOPnrcBLu6/GUzu39wl4rz3Rz4dO8gFFnJCY8cpxK4ADL570fL3bJGrVuF1B8vRbC74r4ww2nTIxOKKfnWvgjocPB+bnaT/MFxRxQmLGdtWdXqYn65j76lVWQj7YLtZrKbfpyTqe2rkdL+2+2riqT29+3h2B20wIYi/wfEERJyRm/ETOa6m1XhG2nQnpXihslnKzaW1r65BhL/D8wcImITET1Ca212EyOCnIFvdCEeRHd50lF1YdrHTGPAuZgH+KxK87IsmeQBEXkZUA/hbA+Z3X71PVO0XkcgDfAbAawLMAPq2qbyUZLCFFoNdbbXKJuI/bjoB76R0N+6Vuei8O880Wqk4F99682VOETY6Wigi+ftMmCneOsUmnnAWwXVU3AdgM4I9FZAuArwG4V1XfC+AUgM8mFyYhxWJ6so6ZHeuMK+oI2qmQMNZCALjgvHN+7tm5BsYM2/fqn+LnLDGlXCjg+SdwJK6qCuB3nV+dzj8FsB3Av+g8/gCAXQC+EX+IhCRDkhNZggqFivYo3KY9bC9uT/CvzD7v2YoWaIuvaXRvGrlzZmZxscqJi0gFwCEA/wTAfwXwMwDzqrrQeckvAHh+2iJyG4DbAGDNmjXDxktILIRtUBWWux4/YmXVs5fvNgpg12NH8Eaz5fneigjuuW6jMZXjV3TlMmrFxErEVXURwGYRqQH4HoD3eb3M8N77AdwPAFNTU2H/ZglJBL+CYBQh6x3V18YdK5eJK6imXLRphO7VVdBlURVf2PscauMOnDFBa6l/aj2dJeUjlMVQVecB/A2ALQBqIuJeBC4FcDze0AhJjjANqoIYtPnZCLhTaXcK3LZ+wvP5Le9Z5TtRx49uDNJuaMVV5suNjTtlAkBLVedFpArgj9Auah4AcAPaDpXPAPh+koESEidhGlSZ6O0xEpbWouKOhw8bR9sv/7qJW7es8ey/4teFcHAfv31zwehIIeXAZiR+MYADIvJTAH8H4K9V9QcAvgTgiyLyDwAuAvCt5MIkJF5sJsD4MdhjJAp+Bc3j803cPb0Rt25Z03W4VERw/RV13HnNButeK4uqyyb/kHJh4075KYBJj8d/DuBDSQRFSNJEdWMMM/oOwyW1KmbnGtj7k1f71sjc+5NXMfXu1d3ipRv7mbcWjKPzYXL9JP+IhrA3DcvU1JQePHgwtf2R/FOkftVhZlc6FcEF563wLUL6vXfPDZuw67EjxqXRnrvzqlCxCYCXdl8dOhaSD0TkkKpOeT3HafckM5K2+cWN7ezKwenpa3c+EW5HnXGV6QLg9bi7L1OenU2rykshRLxIozViT9w2v6QJcq6YVseph1z0obWkkXp2u/sdHJHTWlhuct/F0KZLGykmcdr80sBvNOtn4YsioI35JsYd89fT9Pc/2Euc1sLyk/uReNFGa8SeOGx+w+J1lwd4Fzy9uhPark1ZdcbQbC1Zx1URwflOBWcM7/H7++fMy9Ei9yJetNEascckimnd+nvl5GceOQwIuosIe+Xpg1J7Xtt1KuI5g9KUY19U9V3wmH//xCX3Ip6H0RpJhqybLnnd5fWKrIvNCvOB211UXHBeBUutJSyqdj3fB148acyXiwAm85jIuYJprepg17UbOPoeUXKfEx92UgbJL1kXrMOMZm1e667QYxLl028t9nm+Hz3UwLb1E8aJOx7XE8/n5pstzDxymHWiESX3Is5CTTnJQ8E6zN1c0GujzOBsthbxg8MnsNKngAm0R93d/xteE9XRQopP7tMpAAs1ZSQPBWuvnLwzJn05ccDuzi/KCj2Af0fCLgq83Jmoc7mP55x58tGkECJOykceCtamnLzXY0EXFr+467UqTp9diDR7E+i/CzDViAZfR0YHijjJhLwUrE13eWHvBkzHU69V8dTO7ZEXRHZb1rrM7FiHmX2H++4UgPYdBOtEowlFnGRC1vZCl8Hi6rb1Ezjw4snQxdag4/Ea9fs1rQKAVeMO7rym33Xi/v+ux49030t3ymhDESeZkJW9cHAFnt+9udC1FTbmm3jw6WPd15p6uczONTxF9J7rNvY9fv4K/4Ll1R+4GI8eavQJv6DdOmWw/0ovfjWirB0/JH3YxZCUCj8Ri5rSqIhgSbU7Un/oJ69i0cP/N+6MobWofV5zV5RXDVwwgPZIvdcr7r629/l7rtsIwO5i53V8phmlpFj4dTGkiJNSMDg6dukVMT8Pd1a4OXNTbLWqg7MLS1bCbNqGuw9SXNiKlpQavxF2s7WIXY8dwcFXXs+dgAPnFkk2uVu8HC0mK2YeHD8kfXI/2YeQIII82vPNVl+uO0+4S6+FdeV4CbNpG7QelhuOxElhSWuptCRZVMXlO59AbdzxbJBlWhTZXb5t0FkzWChli4ryQxEnsZKWOyJqkTKPKLBMqF3HC+C9yMO29RPLOiU++PQxVJ0xrBp3MH+mRXfKiEARJ7GR5nJrUae5F4WzC+0+4iYrpun42z3LBffevJniPSKMjIjTP5s8tv1QTJ9FmM+o7MW6oPa3X9j7nNV7SfkZCREv2oK8RcXGHWH6LA6+8npfPjfoM/LrIVIW/C5UQcdf9oscOcdIuFP8RogkPmzcEabP4qFnXg31GW1bPzFktPnHz1Xi1Wff9r2kXIzESJz+2XSw6YdiGj0uGiaduZ/R7FwDux47ErkTYNEIcpV49VCxfS8pFyMxEqd/Nh1sFvCoiGlZA29cK93MI4dHRsArIlZT5acn65j76lW47+bNXDRlhBmJaffsKZEf1vosajC4cLD7GRXdCx4FASIXfUn5GPlp91kvyJsH8iICdZ++2651rjHfREUEzdaiZ7pgFOhdsi5s0ZfkhzS+dyMxEh918nQnEhRLHibxVES6K9KbcvU23HfzZtzx8GHjNpyKAIq+WZp+8QzCxlb5Js7vnd9IfCRy4qNOntw5QXnzPEzi6V2RfhimJ+u+29hzwybsuXFT91wExTMIC/P5Jq3v3UikU0advLlz/BY1KIswrRp3APinj9xz4P40tZI1jcRZmM83aX3vOBIfAZJ258zONbB195O4fOcT2Lr7SczONSJvw2bsu2rcwVg4k0uqOBXBnde0+554+bmdiuD02YVl58vrtVWngluuvMzzcdoI801arjiK+AhgEoc4RMDN+zXmm33FuDBC3ruNIKpOBWdbiwhII2dGRQR7btjUN8ruTR+tGncAbbfHHTxfplTT3dMbA62bJH8k+b3rhYXNESGpKnkcq8n4rbizatyBKvBG81xXvtt9+oZkiU3RiqvvjBZxfe9G3mJI/PPQw+CX97P9AzZtQwDMffUqAOe+DH6Nn9JCBLjkwmrXCrmo2rVIAm2hNh1z3uoTJFmS+t71EijiInIZgL8E8I8BLAG4X1X/TERWA9gLYC2AlwHcpKqnkguV5JHauOPp466NO9ZNx0zNnMZEsHbnExgT5Cp9ogrPUbNNozXTsbJISaJikxNfAHCHqr4PwBYAnxOR9wPYCeDHqvpeAD/u/E5GiNm5Bn735oLnc/PNlrW9ytTMyXVk5EnAgXbqY5DZuQbuePhw4DGnlSclo0PgSFxVTwA40fn/b0XkBQB1AJ8A8Iedlz0A4G8AfCmRKEku2bP/qHGiiqnU4pU2GJxROzbkJJukWXtRv4i7I3AbPzdnD5O4CZUTF5G1ACYBPAPgXR2Bh6qeEJF3Gt5zG4DbAGDNmjXDxEpyRpQ8bm/awCtnDiC3hUuXp372Or4y+zzunt4IIHiC0mCqJI08KRkdrC2GIvI2AI8CuF1Vf2P7PlW9X1WnVHVqYqL8PaBHibB53N60gZc1cWbfYXwx5wLu8tAzr3b/72eNZKqEJI2ViIuIg7aAf1tVv9t5+JcicnHn+YsBvJZMiCSvBC1M0IvN9PrWomIp9iiTwU2dzM41jFPmbVvKEjIMNu4UAfAtAC+o6n/peeoxAJ8BsLvz8/uJREhyS68gB03UGXRzFN1SNyb+/nYB8PWbNlHASeLY5MS3Avg0gOdFxL3X/Xdoi/fDIvJZAMcA3JhMiCTP9OZ33//v/yfOtJaPpd0+Ir0Ufo1M9U+j5LcsS8pGYDpFVf+3qoqqfkBVN3f+/Q9V/bWqfkRV39v5+XoaAZP88h+v+0C7vWoPvX1EesnjGpkVn4Ysg8/YpH3Cth8gJAqcsZlj8rKQgy1h7HMHXjyZdni+jDtjnncRw+B6xPP8mZHiQxHPKTaz/9KIIexFxNY+l7eceJCAR02P5O04w1C0QcSowi6GOSXrhRzi6E7ot+2xkAsmF5WiTqdP8vMn8UIRzylZN0pK6iISNLsxj0S93Hh5xOPovZ4GWQ8iiD0U8ZySVkN5E0ldRKIuv+aMSdflMsyCEFHeeuuWNdZ+eBevnt9FGt1mPYgg9lDEc0rWjZKSuohEEQEBcPOHLsOd12xAvVYdqiGWAqhVl1seTdRr1e6iDBXLFJCg7b4ZzB8XaXSb9SCC2MPCZk7JulHSzI51nit1D3sRieIPVwA/OHwCe//uVbQW00vD9B6ve94Hz4kXCuDBp4/hB4dP9C1mUaTRbVKfP4kfruxDluG6ErwWPRj2IjLouskb9VrV96I5O9eI1KCr6lSw0hnz7L3u7jdv7g+6U/IDV/YhAOy+lIMiu6jaHYHF8QUOM1U/CoL2SHiVYbEKP7yWSDN1WgxLs7WI81eMoepUPC9gWVhIg2C3xWLAnPiIYFtUSyNvOz1Zx1M7t+Pl3VfHsj03V12vVXHrljWo16qYDxBwm3qD6ZxVnWhfmzeare6Cx17kNT9O8g1FfESwFec487Y2drowRUYvalUHP7vnY7jv5s04fXYBDz59rCu6fu+557qNffte6SHMpnO2MqRTxeWSWrV7ATOVSPOYHyf5hiI+ItiKc1yuBJuR/1dmn8d8M1zKoxdnTLDr2g3dfdlsy30PAJxdODdL89SZ1rL4TOcsaJTvxeBIn+4PEhcU8RHBJA5jIn3CFZe1MWjkPzvXwLefPhZqm73Ua1XsubHd6tXGey4W7xm8M/ETWlNKZHCfbqyuZ9y9O2nMN5eNxun+IFFgYXNE8LKMAe3CZW9BLS5ro6lo6T6+Z//RyP1IBP39yYNSEF4Fy6D4gPY5m3nk8LJ1RI937i7cIqoJHdj3YNG4dxsVkb6LSBYFRbpRiglFfERwv4x3PHx42ZT3wW57cbgSKobFjt0i5DC53wsH8uh+3vOqU8G29RPYuvvJPnEKiq+LR/Jae34GCXnvcXqN/t1tuLFk5VLJQ8M1Eg2mU0pOb3Fxz/6jViuyx4FpP+7jw+R+55utvkKpaZm4VeMOrr+ijkcPNfpy87fvfc43Pne7e/YfDZxc5I62TemV3uM0nePBPWThUinSbFLSD0fiJcZrdGUaOcZRUOu9HTeNdF2xM6V3bPEaKXr5ub3uPIJwt2t7YXPvAgbPrVcx09Ybn7ZLpUizSUk/FPESY7p992LYlXa8JgkN4jWN/a7HjyyblBOUonDpTQMNpoCG6Zbobjdsi4CgHLfXhcvvoppmjtp0rHTL5B+mUywpSgvRXsII0LAr7QQ5RLy6+gHAb5oLy17riqENppFi1G6JLo35pjFN44cpxz0718D0ZB3XX1Hv5t0rIviD31vt6Qbatn4i1Y6HWTdcI9GhiFtQpBaiLrNzjVBtV4e9bfZ7/+C0/dm5Bjbf9SPf3LSbawY8io09eI0UZ+casU3p95thacKU456da+DRQ43uMS+q4tljb+D6K+qo16pdG+Q9123EgRdPppqjnp6sd4+1Nw4WNfMPG2BZ4Pp6B/GyruUFU8wmhj2WoP2527dtgDUYj9f73FREvScHvuuxI0NNIDLF4Ld/GwRtV41XbG78g03HTNt5KaZ2BaQ4sAHWkBSx6BM0Mo7aYtSUpzV5qgfjsUlzOBXB6bMLuHznE8tywa7Q9QpoY76JmUcOYwnAok+z8TCi2xsz4J3DX+mMYWFJrdrj1nwacrl3dn71BBfmqMkgTKdYUMQp0qbY3NvkKLfNfmml6ck63rbSPCZw4wm68ElHaeebLc99PLVzO+q16jIxbi2pr4C3Nx54iJ4x9/Jmz4LKzdaSlYBXnQr8bnjdIqjNdpijJoNQxC0oYtHHL2ZXDF/afTWe2rndOu8Z5CX26yninqugC58Ay0bzzdYi7nr8SPf3KHdA9VrVV0i94mjMN/uK2FGKpe5F8g2fFI+Ng4Y5amKCIm5BEYs+ScQclFYyCXSt6nT3G+T4MA2mT51pdcU0yh3QtvUT1surAf2pGvdOIOzFw20PMD1ZN8a8atwJLJy6ufk8/72R7GBhk1gTVOD1Kv5VnQruuW4jgHOTcWrjDlTb/bXHfIp4YfZj895t6yfwYEDTLVPe3BXaqMXioHNjOh73NRTw0cavsMmROLHGNIo+89ZCN2ftNfoH0JdLP3WmhbMLS7j35s1YCjGIcEfC7n7CcHy+ibunN+JTW9b0+bS3/t7qvnhN0RwP6RsfTLf53Rn1PufGBRTjjo9kD0filhSxw1sSMc/ONTxtfH4jRj/7oZ+dbpBB22EUG6XpHPSuK+q3b5s1NmtVB7uu3ZD7vw9SHDgSH5KiTvZJIubpyTouOH+5C8UtPnrNavXLJXsJuFMROGP9+WuvQrLXyNgZEzgV79y36Rz0nisvBtsFBOXWexebICRpKOIWFLHDW5Ixm0T51JmW50XDphBZETm3cMMNm7Dnxk2BRVmvFMWeGzdhzw2bQq1j6ec68dp30J1D3v82SLngZB8LyjTZJ46YbRtDuWI2s2MdZvYd9vVUL6kum4lok44w9T6fnqzj8p1PeOa4B8+B6ZwMLj7hUrc4/jz/bZBywZG4BWWa7BNHzGEKfMfnm+0UzHn+44Uoa3gGNSSzPQdhz5XN8dfGncI1TCPFhIdhrGEAAAjoSURBVCJuQdkm+wyLVxrDtGq9K4R+k13CxmWb77c9B2sv8hZr0+ODbpLBDLlTEfzuzYVC1VBIcWE6xYK41p2MQlSHSdoxf3zTxXj0UMPYk8WUgqmIhLLRzc41rJaYA+zPwdM/P+W5L9Pj7rZ7uzL27uP02YVl7h2v+IaliI4pEj+0GOYYvwkifl/WpL/cpriuv6KOAy+e9Nxv1GMJ2m8vUTv8rd35hPG5lyNsz5SLj7MDYRznkxSHoboYish/B/BxAK+p6u93HlsNYC+AtQBeBnCTqpqHLSQSfg4T0xc1jQVvTXEdePGksZ1tHHcGQb1Loub7/bzq7iSmMKSxSk6Uvw1STmxy4n8B4I8HHtsJ4Meq+l4AP+78TmImisPE1lo4zEpFUZ0vURtv2WxfgMj5/luuvMz4XBSrYBo1lCI6pkgyBIq4qv4tgNcHHv4EgAc6/38AwHTMcRFEc5jYfLmHnQiUlVvHb/uK6Hcad0+bp/APnk+vi9/gYwASb5hWRMcUSYao7pR3qeoJAOj8fKfphSJym4gcFJGDJ08Ot47jqBFlRGfz5R52IlBWbp2ZHeuMLcHDLqFm+/7e8+Z18Zt55DBm9h1edkEEMNRdRxBFdEyRZEjcYqiq96vqlKpOTUwMt6L6qBGlnazNl3vYW/GsWvNOT9Zx65Y1y4Q8DvGyOW9eF7+Wx8o+aczYLGJ7ZJIMUS2GvxSRi1X1hIhcDOC1OIMqC3G4REwzEv1eD/gXEOMovIWNKy7unt6IqXevNh5fkpbMMPnmNHLTWX0GJF9EFfHHAHwGwO7Oz+/HFlFJSMMlYiLoyz2zY52nPS3pW/EoAmt6j6kTYZhz7rVtv8WibdsNuK8lJA1sLIYPAfhDAO8QkV8AuBNt8X5YRD4L4BiAG5MMsojk2QIW50QgW2GOclGzeU/v/oHlCzqYznmUeLwufs6YAIK+lEocF0RO5CG2BIq4qt5ieOojMcdSKvJuARsczboOi7CjZFshjHJRC3qP7Qo/Xuc8Sjymi5/XY8MIbpZ3caR4cNp9QqQx4SMuoopGGCGMclELeo/twsVe53wYr7upa2Jc5PkujuQPNsBKiCJZwKJaDsMIYRRfc9B7bO9qvM55nn3Web+LI/mCIp4QebWAeU1WiSoaYYQwykUt6D02grtq3PE853m9yM7ONTBmWDkoDxcYkj+YTkmQvFnATGmT2riDU2eWt4r1Eo3egtuFVQdORayKelGKqUHv8So09lJ1Krjzmg2Rtu133L2vjbMA6X4+Xn1c8nCBIfmEXQwzJk0Xgmlh4VrVwdmFpcCOeF6FRGdM8LaVKzB/poVLalVsWz9h7GSYBIMXFRF0Y4lr335dG73a70a94zJ9PhURfP2mTbkaEJB0GaqLIUmOtF0IpvTIG80W7r15c+DFxDRjcfy8FZj76lWZuCrSuNsx1QweeuZVq77mtpg+nyVVCjgxQhHPkLRdCH6OGRsxjOIWKYOrwnTcpva1UQuQRXI0kfzAwmaGpO1CGLaYF9UtksTxDNNKNyym467EXIDMa7GV5BuKeIakbXMb1jET1S0S9/EM20o3LKbjvuXKy2IV3bw6mki+YTolQ7LoYTJMDjmKWySJ40k7beN33H7NuKLui6JNwkB3SsaUrUdGGsdjs4Zl2c4rGW3oTskxZRt5pXE8QQVA9h4howRz4qRwBOXmh125iJAiwZE4SYw4+4f3EpSbZ+8RMkpQxEkiJNU/3MUvbUO/NRklmE4hiRAlpRFXGoR+azJKcCROEiGJ/uG2xLlyESF5hyJOEiFKSiPONEjZXD+EmGA6hSRCEv3DCSHL4UicJEIS/cMJIcvhjE1CCMk5fjM2mU4hhJACQxEnhJACQxEnhJACQxEnhJACQxEnhJACk6o7RUROAngltR1G4x0AfpV1ECnA4ywXo3KcwOgca+9xvltVJ7xelKqIFwEROWiy8pQJHme5GJXjBEbnWG2Pk+kUQggpMBRxQggpMBTx5dyfdQApweMsF6NynMDoHKvVcTInTgghBYYjcUIIKTAUcUIIKTAU8R5EpCIicyLyg6xjSRIReVlEnheR50SktG0lRaQmIvtE5EUReUFE/mnWMcWNiKzrfI7uv9+IyO1Zx5UEIvIFETkiIn8vIg+JyMqsY0oCEfl85xiP2HyW7Cfez+cBvADg7VkHkgLbVLXsEyb+DMAPVfUGETkPwHjWAcWNqh4FsBloD0IANAB8L9OgEkBE6gD+LYD3q2pTRB4G8EkAf5FpYDEjIr8P4F8B+BCAtwD8UESeUNX/a3oPR+IdRORSAFcD+GbWsZDhEZG3A/gwgG8BgKq+parz2UaVOB8B8DNVzfus6KisAFAVkRVoX5CPZxxPErwPwNOqekZVFwD8LwD/3O8NFPFz3AfgTwAsZR1ICiiAH4nIIRG5LetgEuI9AE4C+PNOiuybInJB1kElzCcBPJR1EEmgqg0A/xnAMQAnALyhqj/KNqpE+HsAHxaRi0RkHMDHAFzm9waKOAAR+TiA11T1UNaxpMRWVf0ggI8C+JyIfDjrgBJgBYAPAviGqk4COA1gZ7YhJUcnXXQtgEeyjiUJRGQVgE8AuBzAJQAuEJFPZRtV/KjqCwC+BuCvAfwQwGEAC37voYi32QrgWhF5GcB3AGwXkQezDSk5VPV45+draOdPP5RtRInwCwC/UNVnOr/vQ1vUy8pHATyrqr/MOpCE+CMAL6nqSVVtAfgugD/IOKZEUNVvqeoHVfXDAF4HYMyHAxRxAICqfllVL1XVtWjfkj6pqqW7ygOAiFwgIv/I/T+Aq9C+hSsVqvr/ALwqIus6D30EwP/JMKSkuQUlTaV0OAZgi4iMi4ig/Xm+kHFMiSAi7+z8XAPgOgR8rnSnjB7vAvC99vcAKwD8lar+MNuQEuPfAPh2J9XwcwD/MuN4EqGTO/1nAP511rEkhao+IyL7ADyLdnphDuWdfv+oiFwEoAXgc6p6yu/FnHZPCCEFhukUQggpMBRxQggpMBRxQggpMBRxQggpMBRxQggpMBRxQggpMBRxQggpMP8fVR4GbRtSaO8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_rm,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assume that the target function is a linear function\n",
    "$$ y = k * rm + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price(rm, k, b):\n",
    "    return k * rm + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define mean absolute loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ (previous) MSE:loss = \\frac{1}{n} \\sum^{n}_{i=1}{(y_i - \\hat y_i)^2}$$ \n",
    "$$ (current) MAE:loss = \\frac{1}{n} \\sum^{n}_{i=1}{|y_i - \\hat y_i|} = \\frac{1}{n} \\sum^{n}_{i=1}{|y_i - (kx_i + b_i)|}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    return sum(np.abs(y_i - y_hat_i)for y_i,y_hat_i in zip(list(y),list(y_hat)))/len(list(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define partial derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial loss}{\\partial k} = - \\frac{1}{n}\\sum^{n}_{i=1}|x_i|$$\n",
    "$$ \\frac{\\partial loss}{\\partial b} = -1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_derivative_k(x): \n",
    "    gradient = -np.mean(np.abs(list(x)))\n",
    "    return gradient\n",
    "\n",
    "def partial_derivative_b():\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.284634387351779"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_derivative_k(X_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 184.44107461237155, parameters k is -37.96063793271662 and b is 76.6604622295003\n",
      "Iteration 1, the loss is 184.40057798298903, parameters k is -37.95435329832927 and b is 76.6614622295003\n",
      "Iteration 2, the loss is 184.36008135360615, parameters k is -37.948068663941925 and b is 76.6624622295003\n",
      "Iteration 3, the loss is 184.31958472422355, parameters k is -37.941784029554576 and b is 76.66346222950031\n",
      "Iteration 4, the loss is 184.27908809484092, parameters k is -37.93549939516723 and b is 76.66446222950032\n",
      "Iteration 5, the loss is 184.23859146545803, parameters k is -37.92921476077988 and b is 76.66546222950032\n",
      "Iteration 6, the loss is 184.19809483607557, parameters k is -37.92293012639253 and b is 76.66646222950033\n",
      "Iteration 7, the loss is 184.15759820669294, parameters k is -37.91664549200518 and b is 76.66746222950033\n",
      "Iteration 8, the loss is 184.1171015773103, parameters k is -37.91036085761783 and b is 76.66846222950034\n",
      "Iteration 9, the loss is 184.0766049479275, parameters k is -37.90407622323048 and b is 76.66946222950034\n",
      "Iteration 10, the loss is 184.03610831854505, parameters k is -37.897791588843134 and b is 76.67046222950034\n",
      "Iteration 11, the loss is 183.9956116891622, parameters k is -37.891506954455785 and b is 76.67146222950035\n",
      "Iteration 12, the loss is 183.95511505977947, parameters k is -37.885222320068436 and b is 76.67246222950035\n",
      "Iteration 13, the loss is 183.9146184303968, parameters k is -37.87893768568109 and b is 76.67346222950036\n",
      "Iteration 14, the loss is 183.87412180101398, parameters k is -37.87265305129374 and b is 76.67446222950036\n",
      "Iteration 15, the loss is 183.83362517163167, parameters k is -37.86636841690639 and b is 76.67546222950037\n",
      "Iteration 16, the loss is 183.79312854224895, parameters k is -37.86008378251904 and b is 76.67646222950037\n",
      "Iteration 17, the loss is 183.75263191286626, parameters k is -37.85379914813169 and b is 76.67746222950038\n",
      "Iteration 18, the loss is 183.71213528348358, parameters k is -37.84751451374434 and b is 76.67846222950038\n",
      "Iteration 19, the loss is 183.67163865410075, parameters k is -37.841229879356995 and b is 76.67946222950039\n",
      "Iteration 20, the loss is 183.6311420247182, parameters k is -37.834945244969646 and b is 76.68046222950039\n",
      "Iteration 21, the loss is 183.59064539533574, parameters k is -37.8286606105823 and b is 76.6814622295004\n",
      "Iteration 22, the loss is 183.55014876595303, parameters k is -37.82237597619495 and b is 76.6824622295004\n",
      "Iteration 23, the loss is 183.50965213657005, parameters k is -37.8160913418076 and b is 76.6834622295004\n",
      "Iteration 24, the loss is 183.4691555071877, parameters k is -37.80980670742025 and b is 76.68446222950041\n",
      "Iteration 25, the loss is 183.42865887780496, parameters k is -37.8035220730329 and b is 76.68546222950042\n",
      "Iteration 26, the loss is 183.38816224842216, parameters k is -37.79723743864555 and b is 76.68646222950042\n",
      "Iteration 27, the loss is 183.3476656190393, parameters k is -37.790952804258204 and b is 76.68746222950043\n",
      "Iteration 28, the loss is 183.30716898965687, parameters k is -37.784668169870855 and b is 76.68846222950043\n",
      "Iteration 29, the loss is 183.266672360274, parameters k is -37.778383535483506 and b is 76.68946222950044\n",
      "Iteration 30, the loss is 183.22617573089124, parameters k is -37.77209890109616 and b is 76.69046222950044\n",
      "Iteration 31, the loss is 183.1856791015088, parameters k is -37.76581426670881 and b is 76.69146222950044\n",
      "Iteration 32, the loss is 183.145182472126, parameters k is -37.75952963232146 and b is 76.69246222950045\n",
      "Iteration 33, the loss is 183.10468584274355, parameters k is -37.75324499793411 and b is 76.69346222950045\n",
      "Iteration 34, the loss is 183.0641892133609, parameters k is -37.74696036354676 and b is 76.69446222950046\n",
      "Iteration 35, the loss is 183.02369258397812, parameters k is -37.74067572915941 and b is 76.69546222950046\n",
      "Iteration 36, the loss is 182.98319595459552, parameters k is -37.734391094772064 and b is 76.69646222950047\n",
      "Iteration 37, the loss is 182.9426993252128, parameters k is -37.728106460384716 and b is 76.69746222950047\n",
      "Iteration 38, the loss is 182.90220269583017, parameters k is -37.72182182599737 and b is 76.69846222950048\n",
      "Iteration 39, the loss is 182.8617060664476, parameters k is -37.71553719161002 and b is 76.69946222950048\n",
      "Iteration 40, the loss is 182.8212094370648, parameters k is -37.70925255722267 and b is 76.70046222950049\n",
      "Iteration 41, the loss is 182.78071280768197, parameters k is -37.70296792283532 and b is 76.70146222950049\n",
      "Iteration 42, the loss is 182.7402161782994, parameters k is -37.69668328844797 and b is 76.7024622295005\n",
      "Iteration 43, the loss is 182.69971954891662, parameters k is -37.69039865406062 and b is 76.7034622295005\n",
      "Iteration 44, the loss is 182.65922291953402, parameters k is -37.684114019673274 and b is 76.7044622295005\n",
      "Iteration 45, the loss is 182.61872629015127, parameters k is -37.677829385285925 and b is 76.70546222950051\n",
      "Iteration 46, the loss is 182.57822966076876, parameters k is -37.671544750898576 and b is 76.70646222950052\n",
      "Iteration 47, the loss is 182.53773303138587, parameters k is -37.66526011651123 and b is 76.70746222950052\n",
      "Iteration 48, the loss is 182.49723640200332, parameters k is -37.65897548212388 and b is 76.70846222950053\n",
      "Iteration 49, the loss is 182.45673977262064, parameters k is -37.65269084773653 and b is 76.70946222950053\n",
      "Iteration 50, the loss is 182.4162431432382, parameters k is -37.64640621334918 and b is 76.71046222950054\n",
      "Iteration 51, the loss is 182.37574651385535, parameters k is -37.64012157896183 and b is 76.71146222950054\n",
      "Iteration 52, the loss is 182.3352498844728, parameters k is -37.63383694457448 and b is 76.71246222950055\n",
      "Iteration 53, the loss is 182.29475325508994, parameters k is -37.627552310187134 and b is 76.71346222950055\n",
      "Iteration 54, the loss is 182.25425662570748, parameters k is -37.621267675799785 and b is 76.71446222950055\n",
      "Iteration 55, the loss is 182.21375999632474, parameters k is -37.61498304141244 and b is 76.71546222950056\n",
      "Iteration 56, the loss is 182.17326336694202, parameters k is -37.60869840702509 and b is 76.71646222950056\n",
      "Iteration 57, the loss is 182.13276673755934, parameters k is -37.60241377263774 and b is 76.71746222950057\n",
      "Iteration 58, the loss is 182.09227010817648, parameters k is -37.59612913825039 and b is 76.71846222950057\n",
      "Iteration 59, the loss is 182.05177347879396, parameters k is -37.58984450386304 and b is 76.71946222950058\n",
      "Iteration 60, the loss is 182.01127684941133, parameters k is -37.58355986947569 and b is 76.72046222950058\n",
      "Iteration 61, the loss is 181.9707802200287, parameters k is -37.577275235088344 and b is 76.72146222950059\n",
      "Iteration 62, the loss is 181.9302835906461, parameters k is -37.570990600700995 and b is 76.7224622295006\n",
      "Iteration 63, the loss is 181.88978696126307, parameters k is -37.564705966313646 and b is 76.7234622295006\n",
      "Iteration 64, the loss is 181.84929033188078, parameters k is -37.5584213319263 and b is 76.7244622295006\n",
      "Iteration 65, the loss is 181.80879370249772, parameters k is -37.55213669753895 and b is 76.72546222950061\n",
      "Iteration 66, the loss is 181.76829707311543, parameters k is -37.5458520631516 and b is 76.72646222950061\n",
      "Iteration 67, the loss is 181.72780044373272, parameters k is -37.53956742876425 and b is 76.72746222950062\n",
      "Iteration 68, the loss is 181.68730381435006, parameters k is -37.5332827943769 and b is 76.72846222950062\n",
      "Iteration 69, the loss is 181.64680718496749, parameters k is -37.52699815998955 and b is 76.72946222950063\n",
      "Iteration 70, the loss is 181.60631055558466, parameters k is -37.520713525602204 and b is 76.73046222950063\n",
      "Iteration 71, the loss is 181.5658139262021, parameters k is -37.514428891214855 and b is 76.73146222950064\n",
      "Iteration 72, the loss is 181.52531729681934, parameters k is -37.50814425682751 and b is 76.73246222950064\n",
      "Iteration 73, the loss is 181.48482066743662, parameters k is -37.50185962244016 and b is 76.73346222950065\n",
      "Iteration 74, the loss is 181.44432403805396, parameters k is -37.49557498805281 and b is 76.73446222950065\n",
      "Iteration 75, the loss is 181.40382740867122, parameters k is -37.48929035366546 and b is 76.73546222950065\n",
      "Iteration 76, the loss is 181.36333077928876, parameters k is -37.48300571927811 and b is 76.73646222950066\n",
      "Iteration 77, the loss is 181.32283414990584, parameters k is -37.47672108489076 and b is 76.73746222950066\n",
      "Iteration 78, the loss is 181.2823375205235, parameters k is -37.47043645050341 and b is 76.73846222950067\n",
      "Iteration 79, the loss is 181.2418408911405, parameters k is -37.464151816116065 and b is 76.73946222950067\n",
      "Iteration 80, the loss is 181.2013442617579, parameters k is -37.457867181728716 and b is 76.74046222950068\n",
      "Iteration 81, the loss is 181.16084763237518, parameters k is -37.45158254734137 and b is 76.74146222950068\n",
      "Iteration 82, the loss is 181.12035100299252, parameters k is -37.44529791295402 and b is 76.74246222950069\n",
      "Iteration 83, the loss is 181.07985437360986, parameters k is -37.43901327856667 and b is 76.7434622295007\n",
      "Iteration 84, the loss is 181.0393577442273, parameters k is -37.43272864417932 and b is 76.7444622295007\n",
      "Iteration 85, the loss is 180.9988611148446, parameters k is -37.42644400979197 and b is 76.7454622295007\n",
      "Iteration 86, the loss is 180.95836448546183, parameters k is -37.42015937540462 and b is 76.74646222950071\n",
      "Iteration 87, the loss is 180.91786785607894, parameters k is -37.413874741017274 and b is 76.74746222950071\n",
      "Iteration 88, the loss is 180.8773712266966, parameters k is -37.407590106629925 and b is 76.74846222950072\n",
      "Iteration 89, the loss is 180.8368745973139, parameters k is -37.401305472242576 and b is 76.74946222950072\n",
      "Iteration 90, the loss is 180.79637796793114, parameters k is -37.39502083785523 and b is 76.75046222950073\n",
      "Iteration 91, the loss is 180.75588133854868, parameters k is -37.38873620346788 and b is 76.75146222950073\n",
      "Iteration 92, the loss is 180.71538470916593, parameters k is -37.38245156908053 and b is 76.75246222950074\n",
      "Iteration 93, the loss is 180.67488807978327, parameters k is -37.37616693469318 and b is 76.75346222950074\n",
      "Iteration 94, the loss is 180.63439145040076, parameters k is -37.36988230030583 and b is 76.75446222950075\n",
      "Iteration 95, the loss is 180.59389482101784, parameters k is -37.36359766591848 and b is 76.75546222950075\n",
      "Iteration 96, the loss is 180.55339819163535, parameters k is -37.357313031531135 and b is 76.75646222950076\n",
      "Iteration 97, the loss is 180.51290156225267, parameters k is -37.351028397143786 and b is 76.75746222950076\n",
      "Iteration 98, the loss is 180.47240493286992, parameters k is -37.34474376275644 and b is 76.75846222950076\n",
      "Iteration 99, the loss is 180.4319083034872, parameters k is -37.33845912836909 and b is 76.75946222950077\n",
      "Iteration 100, the loss is 180.39141167410457, parameters k is -37.33217449398174 and b is 76.76046222950077\n",
      "Iteration 101, the loss is 180.3509150447218, parameters k is -37.32588985959439 and b is 76.76146222950078\n",
      "Iteration 102, the loss is 180.31041841533892, parameters k is -37.31960522520704 and b is 76.76246222950078\n",
      "Iteration 103, the loss is 180.2699217859564, parameters k is -37.31332059081969 and b is 76.76346222950079\n",
      "Iteration 104, the loss is 180.22942515657385, parameters k is -37.307035956432344 and b is 76.7644622295008\n",
      "Iteration 105, the loss is 180.1889285271913, parameters k is -37.300751322044995 and b is 76.7654622295008\n",
      "Iteration 106, the loss is 180.14843189780836, parameters k is -37.294466687657646 and b is 76.7664622295008\n",
      "Iteration 107, the loss is 180.10793526842593, parameters k is -37.2881820532703 and b is 76.76746222950081\n",
      "Iteration 108, the loss is 180.0674386390431, parameters k is -37.28189741888295 and b is 76.76846222950081\n",
      "Iteration 109, the loss is 180.02694200966062, parameters k is -37.2756127844956 and b is 76.76946222950082\n",
      "Iteration 110, the loss is 179.98644538027796, parameters k is -37.26932815010825 and b is 76.77046222950082\n",
      "Iteration 111, the loss is 179.94594875089518, parameters k is -37.2630435157209 and b is 76.77146222950083\n",
      "Iteration 112, the loss is 179.9054521215126, parameters k is -37.25675888133355 and b is 76.77246222950083\n",
      "Iteration 113, the loss is 179.8649554921299, parameters k is -37.250474246946204 and b is 76.77346222950084\n",
      "Iteration 114, the loss is 179.82445886274715, parameters k is -37.244189612558856 and b is 76.77446222950084\n",
      "Iteration 115, the loss is 179.78396223336463, parameters k is -37.23790497817151 and b is 76.77546222950085\n",
      "Iteration 116, the loss is 179.74346560398155, parameters k is -37.23162034378416 and b is 76.77646222950085\n",
      "Iteration 117, the loss is 179.7029689745991, parameters k is -37.22533570939681 and b is 76.77746222950086\n",
      "Iteration 118, the loss is 179.6624723452165, parameters k is -37.21905107500946 and b is 76.77846222950086\n",
      "Iteration 119, the loss is 179.62197571583383, parameters k is -37.21276644062211 and b is 76.77946222950087\n",
      "Iteration 120, the loss is 179.58147908645108, parameters k is -37.20648180623476 and b is 76.78046222950087\n",
      "Iteration 121, the loss is 179.54098245706848, parameters k is -37.200197171847414 and b is 76.78146222950087\n",
      "Iteration 122, the loss is 179.50048582768557, parameters k is -37.193912537460065 and b is 76.78246222950088\n",
      "Iteration 123, the loss is 179.4599891983031, parameters k is -37.187627903072716 and b is 76.78346222950088\n",
      "Iteration 124, the loss is 179.4194925689203, parameters k is -37.18134326868537 and b is 76.78446222950089\n",
      "Iteration 125, the loss is 179.3789959395378, parameters k is -37.17505863429802 and b is 76.7854622295009\n",
      "Iteration 126, the loss is 179.33849931015504, parameters k is -37.16877399991067 and b is 76.7864622295009\n",
      "Iteration 127, the loss is 179.29800268077238, parameters k is -37.16248936552332 and b is 76.7874622295009\n",
      "Iteration 128, the loss is 179.25750605138975, parameters k is -37.15620473113597 and b is 76.78846222950091\n",
      "Iteration 129, the loss is 179.21700942200712, parameters k is -37.14992009674862 and b is 76.78946222950091\n",
      "Iteration 130, the loss is 179.17651279262452, parameters k is -37.143635462361274 and b is 76.79046222950092\n",
      "Iteration 131, the loss is 179.13601616324158, parameters k is -37.137350827973926 and b is 76.79146222950092\n",
      "Iteration 132, the loss is 179.09551953385932, parameters k is -37.13106619358658 and b is 76.79246222950093\n",
      "Iteration 133, the loss is 179.0550229044764, parameters k is -37.12478155919923 and b is 76.79346222950093\n",
      "Iteration 134, the loss is 179.01452627509389, parameters k is -37.11849692481188 and b is 76.79446222950094\n",
      "Iteration 135, the loss is 178.97402964571094, parameters k is -37.11221229042453 and b is 76.79546222950094\n",
      "Iteration 136, the loss is 178.93353301632825, parameters k is -37.10592765603718 and b is 76.79646222950095\n",
      "Iteration 137, the loss is 178.89303638694574, parameters k is -37.09964302164983 and b is 76.79746222950095\n",
      "Iteration 138, the loss is 178.85253975756322, parameters k is -37.093358387262484 and b is 76.79846222950096\n",
      "Iteration 139, the loss is 178.81204312818033, parameters k is -37.087073752875135 and b is 76.79946222950096\n",
      "Iteration 140, the loss is 178.77154649879765, parameters k is -37.080789118487786 and b is 76.80046222950097\n",
      "Iteration 141, the loss is 178.73104986941496, parameters k is -37.07450448410044 and b is 76.80146222950097\n",
      "Iteration 142, the loss is 178.6905532400325, parameters k is -37.06821984971309 and b is 76.80246222950097\n",
      "Iteration 143, the loss is 178.65005661064956, parameters k is -37.06193521532574 and b is 76.80346222950098\n",
      "Iteration 144, the loss is 178.60955998126698, parameters k is -37.05565058093839 and b is 76.80446222950098\n",
      "Iteration 145, the loss is 178.5690633518842, parameters k is -37.04936594655104 and b is 76.80546222950099\n",
      "Iteration 146, the loss is 178.52856672250172, parameters k is -37.04308131216369 and b is 76.806462229501\n",
      "Iteration 147, the loss is 178.4880700931189, parameters k is -37.036796677776344 and b is 76.807462229501\n",
      "Iteration 148, the loss is 178.44757346373632, parameters k is -37.030512043388995 and b is 76.808462229501\n",
      "Iteration 149, the loss is 178.40707683435375, parameters k is -37.02422740900165 and b is 76.80946222950101\n",
      "Iteration 150, the loss is 178.36658020497086, parameters k is -37.0179427746143 and b is 76.81046222950101\n",
      "Iteration 151, the loss is 178.32608357558814, parameters k is -37.01165814022695 and b is 76.81146222950102\n",
      "Iteration 152, the loss is 178.28558694620563, parameters k is -37.0053735058396 and b is 76.81246222950102\n",
      "Iteration 153, the loss is 178.2450903168228, parameters k is -36.99908887145225 and b is 76.81346222950103\n",
      "Iteration 154, the loss is 178.20459368744017, parameters k is -36.9928042370649 and b is 76.81446222950103\n",
      "Iteration 155, the loss is 178.16409705805776, parameters k is -36.986519602677554 and b is 76.81546222950104\n",
      "Iteration 156, the loss is 178.1236004286749, parameters k is -36.980234968290205 and b is 76.81646222950104\n",
      "Iteration 157, the loss is 178.08310379929236, parameters k is -36.973950333902856 and b is 76.81746222950105\n",
      "Iteration 158, the loss is 178.0426071699096, parameters k is -36.96766569951551 and b is 76.81846222950105\n",
      "Iteration 159, the loss is 178.002110540527, parameters k is -36.96138106512816 and b is 76.81946222950106\n",
      "Iteration 160, the loss is 177.96161391114424, parameters k is -36.95509643074081 and b is 76.82046222950106\n",
      "Iteration 161, the loss is 177.92111728176167, parameters k is -36.94881179635346 and b is 76.82146222950107\n",
      "Iteration 162, the loss is 177.8806206523788, parameters k is -36.94252716196611 and b is 76.82246222950107\n",
      "Iteration 163, the loss is 177.84012402299618, parameters k is -36.93624252757876 and b is 76.82346222950108\n",
      "Iteration 164, the loss is 177.79962739361355, parameters k is -36.929957893191414 and b is 76.82446222950108\n",
      "Iteration 165, the loss is 177.7591307642308, parameters k is -36.923673258804065 and b is 76.82546222950108\n",
      "Iteration 166, the loss is 177.718634134848, parameters k is -36.917388624416716 and b is 76.82646222950109\n",
      "Iteration 167, the loss is 177.6781375054658, parameters k is -36.91110399002937 and b is 76.8274622295011\n",
      "Iteration 168, the loss is 177.63764087608317, parameters k is -36.90481935564202 and b is 76.8284622295011\n",
      "Iteration 169, the loss is 177.59714424670017, parameters k is -36.89853472125467 and b is 76.8294622295011\n",
      "Iteration 170, the loss is 177.55664761731765, parameters k is -36.89225008686732 and b is 76.83046222950111\n",
      "Iteration 171, the loss is 177.51615098793494, parameters k is -36.88596545247997 and b is 76.83146222950111\n",
      "Iteration 172, the loss is 177.47565435855225, parameters k is -36.87968081809262 and b is 76.83246222950112\n",
      "Iteration 173, the loss is 177.43515772916965, parameters k is -36.873396183705275 and b is 76.83346222950112\n",
      "Iteration 174, the loss is 177.39466109978673, parameters k is -36.867111549317926 and b is 76.83446222950113\n",
      "Iteration 175, the loss is 177.35416447040427, parameters k is -36.86082691493058 and b is 76.83546222950113\n",
      "Iteration 176, the loss is 177.3136678410213, parameters k is -36.85454228054323 and b is 76.83646222950114\n",
      "Iteration 177, the loss is 177.273171211639, parameters k is -36.84825764615588 and b is 76.83746222950114\n",
      "Iteration 178, the loss is 177.23267458225632, parameters k is -36.84197301176853 and b is 76.83846222950115\n",
      "Iteration 179, the loss is 177.19217795287352, parameters k is -36.83568837738118 and b is 76.83946222950115\n",
      "Iteration 180, the loss is 177.15168132349055, parameters k is -36.82940374299383 and b is 76.84046222950116\n",
      "Iteration 181, the loss is 177.11118469410818, parameters k is -36.823119108606484 and b is 76.84146222950116\n",
      "Iteration 182, the loss is 177.0706880647254, parameters k is -36.816834474219135 and b is 76.84246222950117\n",
      "Iteration 183, the loss is 177.03019143534306, parameters k is -36.810549839831786 and b is 76.84346222950117\n",
      "Iteration 184, the loss is 176.98969480596014, parameters k is -36.80426520544444 and b is 76.84446222950118\n",
      "Iteration 185, the loss is 176.94919817657745, parameters k is -36.79798057105709 and b is 76.84546222950118\n",
      "Iteration 186, the loss is 176.908701547195, parameters k is -36.79169593666974 and b is 76.84646222950118\n",
      "Iteration 187, the loss is 176.86820491781208, parameters k is -36.78541130228239 and b is 76.84746222950119\n",
      "Iteration 188, the loss is 176.82770828842933, parameters k is -36.77912666789504 and b is 76.8484622295012\n",
      "Iteration 189, the loss is 176.7872116590469, parameters k is -36.77284203350769 and b is 76.8494622295012\n",
      "Iteration 190, the loss is 176.74671502966424, parameters k is -36.766557399120344 and b is 76.8504622295012\n",
      "Iteration 191, the loss is 176.70621840028133, parameters k is -36.760272764732996 and b is 76.85146222950121\n",
      "Iteration 192, the loss is 176.66572177089893, parameters k is -36.75398813034565 and b is 76.85246222950121\n",
      "Iteration 193, the loss is 176.62522514151615, parameters k is -36.7477034959583 and b is 76.85346222950122\n",
      "Iteration 194, the loss is 176.5847285121336, parameters k is -36.74141886157095 and b is 76.85446222950122\n",
      "Iteration 195, the loss is 176.5442318827507, parameters k is -36.7351342271836 and b is 76.85546222950123\n",
      "Iteration 196, the loss is 176.50373525336798, parameters k is -36.72884959279625 and b is 76.85646222950123\n",
      "Iteration 197, the loss is 176.46323862398557, parameters k is -36.7225649584089 and b is 76.85746222950124\n",
      "Iteration 198, the loss is 176.42274199460275, parameters k is -36.716280324021554 and b is 76.85846222950124\n",
      "Iteration 199, the loss is 176.3822453652202, parameters k is -36.709995689634205 and b is 76.85946222950125\n",
      "Iteration 200, the loss is 176.34174873583768, parameters k is -36.703711055246856 and b is 76.86046222950125\n",
      "Iteration 201, the loss is 176.30125210645468, parameters k is -36.69742642085951 and b is 76.86146222950126\n",
      "Iteration 202, the loss is 176.2607554770721, parameters k is -36.69114178647216 and b is 76.86246222950126\n",
      "Iteration 203, the loss is 176.22025884768942, parameters k is -36.68485715208481 and b is 76.86346222950127\n",
      "Iteration 204, the loss is 176.1797622183067, parameters k is -36.67857251769746 and b is 76.86446222950127\n",
      "Iteration 205, the loss is 176.13926558892385, parameters k is -36.67228788331011 and b is 76.86546222950128\n",
      "Iteration 206, the loss is 176.09876895954142, parameters k is -36.66600324892276 and b is 76.86646222950128\n",
      "Iteration 207, the loss is 176.05827233015876, parameters k is -36.659718614535414 and b is 76.86746222950129\n",
      "Iteration 208, the loss is 176.0177757007761, parameters k is -36.653433980148066 and b is 76.86846222950129\n",
      "Iteration 209, the loss is 175.97727907139327, parameters k is -36.64714934576072 and b is 76.8694622295013\n",
      "Iteration 210, the loss is 175.93678244201055, parameters k is -36.64086471137337 and b is 76.8704622295013\n",
      "Iteration 211, the loss is 175.8962858126282, parameters k is -36.63458007698602 and b is 76.8714622295013\n",
      "Iteration 212, the loss is 175.8557891832454, parameters k is -36.62829544259867 and b is 76.87246222950131\n",
      "Iteration 213, the loss is 175.8152925538627, parameters k is -36.62201080821132 and b is 76.87346222950131\n",
      "Iteration 214, the loss is 175.77479592447995, parameters k is -36.61572617382397 and b is 76.87446222950132\n",
      "Iteration 215, the loss is 175.73429929509734, parameters k is -36.609441539436624 and b is 76.87546222950132\n",
      "Iteration 216, the loss is 175.69380266571466, parameters k is -36.603156905049275 and b is 76.87646222950133\n",
      "Iteration 217, the loss is 175.65330603633194, parameters k is -36.596872270661926 and b is 76.87746222950133\n",
      "Iteration 218, the loss is 175.61280940694937, parameters k is -36.59058763627458 and b is 76.87846222950134\n",
      "Iteration 219, the loss is 175.57231277756657, parameters k is -36.58430300188723 and b is 76.87946222950134\n",
      "Iteration 220, the loss is 175.53181614818402, parameters k is -36.57801836749988 and b is 76.88046222950135\n",
      "Iteration 221, the loss is 175.49131951880125, parameters k is -36.57173373311253 and b is 76.88146222950135\n",
      "Iteration 222, the loss is 175.45082288941887, parameters k is -36.56544909872518 and b is 76.88246222950136\n",
      "Iteration 223, the loss is 175.41032626003593, parameters k is -36.55916446433783 and b is 76.88346222950136\n",
      "Iteration 224, the loss is 175.36982963065333, parameters k is -36.552879829950484 and b is 76.88446222950137\n",
      "Iteration 225, the loss is 175.32933300127058, parameters k is -36.546595195563135 and b is 76.88546222950137\n",
      "Iteration 226, the loss is 175.2888363718882, parameters k is -36.54031056117579 and b is 76.88646222950138\n",
      "Iteration 227, the loss is 175.24833974250538, parameters k is -36.53402592678844 and b is 76.88746222950138\n",
      "Iteration 228, the loss is 175.20784311312258, parameters k is -36.52774129240109 and b is 76.88846222950139\n",
      "Iteration 229, the loss is 175.16734648374006, parameters k is -36.52145665801374 and b is 76.88946222950139\n",
      "Iteration 230, the loss is 175.12684985435743, parameters k is -36.51517202362639 and b is 76.8904622295014\n",
      "Iteration 231, the loss is 175.08635322497472, parameters k is -36.50888738923904 and b is 76.8914622295014\n",
      "Iteration 232, the loss is 175.04585659559208, parameters k is -36.502602754851694 and b is 76.8924622295014\n",
      "Iteration 233, the loss is 175.00535996620934, parameters k is -36.496318120464345 and b is 76.89346222950141\n",
      "Iteration 234, the loss is 174.96486333682674, parameters k is -36.490033486076996 and b is 76.89446222950141\n",
      "Iteration 235, the loss is 174.9243667074441, parameters k is -36.48374885168965 and b is 76.89546222950142\n",
      "Iteration 236, the loss is 174.88387007806125, parameters k is -36.4774642173023 and b is 76.89646222950142\n",
      "Iteration 237, the loss is 174.8433734486786, parameters k is -36.47117958291495 and b is 76.89746222950143\n",
      "Iteration 238, the loss is 174.802876819296, parameters k is -36.4648949485276 and b is 76.89846222950143\n",
      "Iteration 239, the loss is 174.76238018991322, parameters k is -36.45861031414025 and b is 76.89946222950144\n",
      "Iteration 240, the loss is 174.7218835605306, parameters k is -36.4523256797529 and b is 76.90046222950144\n",
      "Iteration 241, the loss is 174.68138693114776, parameters k is -36.446041045365554 and b is 76.90146222950145\n",
      "Iteration 242, the loss is 174.6408903017654, parameters k is -36.439756410978205 and b is 76.90246222950145\n",
      "Iteration 243, the loss is 174.6003936723825, parameters k is -36.43347177659086 and b is 76.90346222950146\n",
      "Iteration 244, the loss is 174.55989704299975, parameters k is -36.42718714220351 and b is 76.90446222950146\n",
      "Iteration 245, the loss is 174.5194004136171, parameters k is -36.42090250781616 and b is 76.90546222950147\n",
      "Iteration 246, the loss is 174.47890378423455, parameters k is -36.41461787342881 and b is 76.90646222950147\n",
      "Iteration 247, the loss is 174.43840715485194, parameters k is -36.40833323904146 and b is 76.90746222950148\n",
      "Iteration 248, the loss is 174.39791052546937, parameters k is -36.40204860465411 and b is 76.90846222950148\n",
      "Iteration 249, the loss is 174.35741389608654, parameters k is -36.39576397026676 and b is 76.90946222950149\n",
      "Iteration 250, the loss is 174.31691726670397, parameters k is -36.389479335879415 and b is 76.91046222950149\n",
      "Iteration 251, the loss is 174.27642063732102, parameters k is -36.383194701492066 and b is 76.9114622295015\n",
      "Iteration 252, the loss is 174.23592400793862, parameters k is -36.37691006710472 and b is 76.9124622295015\n",
      "Iteration 253, the loss is 174.19542737855588, parameters k is -36.37062543271737 and b is 76.9134622295015\n",
      "Iteration 254, the loss is 174.1549307491733, parameters k is -36.36434079833002 and b is 76.91446222950151\n",
      "Iteration 255, the loss is 174.11443411979047, parameters k is -36.35805616394267 and b is 76.91546222950151\n",
      "Iteration 256, the loss is 174.07393749040784, parameters k is -36.35177152955532 and b is 76.91646222950152\n",
      "Iteration 257, the loss is 174.0334408610251, parameters k is -36.34548689516797 and b is 76.91746222950152\n",
      "Iteration 258, the loss is 173.99294423164275, parameters k is -36.339202260780624 and b is 76.91846222950153\n",
      "Iteration 259, the loss is 173.95244760225998, parameters k is -36.332917626393275 and b is 76.91946222950153\n",
      "Iteration 260, the loss is 173.9119509728773, parameters k is -36.326632992005926 and b is 76.92046222950154\n",
      "Iteration 261, the loss is 173.87145434349455, parameters k is -36.32034835761858 and b is 76.92146222950154\n",
      "Iteration 262, the loss is 173.83095771411183, parameters k is -36.31406372323123 and b is 76.92246222950155\n",
      "Iteration 263, the loss is 173.79046108472932, parameters k is -36.30777908884388 and b is 76.92346222950155\n",
      "Iteration 264, the loss is 173.7499644553466, parameters k is -36.30149445445653 and b is 76.92446222950156\n",
      "Iteration 265, the loss is 173.70946782596374, parameters k is -36.29520982006918 and b is 76.92546222950156\n",
      "Iteration 266, the loss is 173.6689711965811, parameters k is -36.28892518568183 and b is 76.92646222950157\n",
      "Iteration 267, the loss is 173.62847456719862, parameters k is -36.282640551294485 and b is 76.92746222950157\n",
      "Iteration 268, the loss is 173.5879779378158, parameters k is -36.276355916907136 and b is 76.92846222950158\n",
      "Iteration 269, the loss is 173.54748130843322, parameters k is -36.27007128251979 and b is 76.92946222950158\n",
      "Iteration 270, the loss is 173.5069846790505, parameters k is -36.26378664813244 and b is 76.93046222950159\n",
      "Iteration 271, the loss is 173.46648804966787, parameters k is -36.25750201374509 and b is 76.93146222950159\n",
      "Iteration 272, the loss is 173.42599142028507, parameters k is -36.25121737935774 and b is 76.9324622295016\n",
      "Iteration 273, the loss is 173.38549479090233, parameters k is -36.24493274497039 and b is 76.9334622295016\n",
      "Iteration 274, the loss is 173.3449981615197, parameters k is -36.23864811058304 and b is 76.9344622295016\n",
      "Iteration 275, the loss is 173.30450153213695, parameters k is -36.232363476195694 and b is 76.93546222950161\n",
      "Iteration 276, the loss is 173.26400490275438, parameters k is -36.226078841808345 and b is 76.93646222950161\n",
      "Iteration 277, the loss is 173.22350827337155, parameters k is -36.219794207420996 and b is 76.93746222950162\n",
      "Iteration 278, the loss is 173.18301164398895, parameters k is -36.21350957303365 and b is 76.93846222950162\n",
      "Iteration 279, the loss is 173.14251501460643, parameters k is -36.2072249386463 and b is 76.93946222950163\n",
      "Iteration 280, the loss is 173.1020183852239, parameters k is -36.20094030425895 and b is 76.94046222950163\n",
      "Iteration 281, the loss is 173.06152175584117, parameters k is -36.1946556698716 and b is 76.94146222950164\n",
      "Iteration 282, the loss is 173.02102512645843, parameters k is -36.18837103548425 and b is 76.94246222950164\n",
      "Iteration 283, the loss is 172.98052849707574, parameters k is -36.1820864010969 and b is 76.94346222950165\n",
      "Iteration 284, the loss is 172.94003186769294, parameters k is -36.175801766709554 and b is 76.94446222950165\n",
      "Iteration 285, the loss is 172.89953523831025, parameters k is -36.169517132322206 and b is 76.94546222950166\n",
      "Iteration 286, the loss is 172.85903860892793, parameters k is -36.16323249793486 and b is 76.94646222950166\n",
      "Iteration 287, the loss is 172.818541979545, parameters k is -36.15694786354751 and b is 76.94746222950167\n",
      "Iteration 288, the loss is 172.77804535016259, parameters k is -36.15066322916016 and b is 76.94846222950167\n",
      "Iteration 289, the loss is 172.73754872077976, parameters k is -36.14437859477281 and b is 76.94946222950168\n",
      "Iteration 290, the loss is 172.69705209139707, parameters k is -36.13809396038546 and b is 76.95046222950168\n",
      "Iteration 291, the loss is 172.65655546201438, parameters k is -36.13180932599811 and b is 76.95146222950169\n",
      "Iteration 292, the loss is 172.61605883263186, parameters k is -36.125524691610764 and b is 76.95246222950169\n",
      "Iteration 293, the loss is 172.57556220324918, parameters k is -36.119240057223415 and b is 76.9534622295017\n",
      "Iteration 294, the loss is 172.5350655738667, parameters k is -36.112955422836066 and b is 76.9544622295017\n",
      "Iteration 295, the loss is 172.49456894448377, parameters k is -36.10667078844872 and b is 76.9554622295017\n",
      "Iteration 296, the loss is 172.45407231510092, parameters k is -36.10038615406137 and b is 76.95646222950171\n",
      "Iteration 297, the loss is 172.4135756857183, parameters k is -36.09410151967402 and b is 76.95746222950172\n",
      "Iteration 298, the loss is 172.3730790563357, parameters k is -36.08781688528667 and b is 76.95846222950172\n",
      "Iteration 299, the loss is 172.3325824269533, parameters k is -36.08153225089932 and b is 76.95946222950172\n",
      "Iteration 300, the loss is 172.29208579757028, parameters k is -36.07524761651197 and b is 76.96046222950173\n",
      "Iteration 301, the loss is 172.25158916818773, parameters k is -36.068962982124624 and b is 76.96146222950173\n",
      "Iteration 302, the loss is 172.211092538805, parameters k is -36.062678347737275 and b is 76.96246222950174\n",
      "Iteration 303, the loss is 172.17059590942236, parameters k is -36.05639371334993 and b is 76.96346222950174\n",
      "Iteration 304, the loss is 172.13009928003973, parameters k is -36.05010907896258 and b is 76.96446222950175\n",
      "Iteration 305, the loss is 172.08960265065693, parameters k is -36.04382444457523 and b is 76.96546222950175\n",
      "Iteration 306, the loss is 172.04910602127416, parameters k is -36.03753981018788 and b is 76.96646222950176\n",
      "Iteration 307, the loss is 172.00860939189175, parameters k is -36.03125517580053 and b is 76.96746222950176\n",
      "Iteration 308, the loss is 171.96811276250907, parameters k is -36.02497054141318 and b is 76.96846222950177\n",
      "Iteration 309, the loss is 171.92761613312607, parameters k is -36.018685907025834 and b is 76.96946222950177\n",
      "Iteration 310, the loss is 171.88711950374366, parameters k is -36.012401272638485 and b is 76.97046222950178\n",
      "Iteration 311, the loss is 171.84662287436097, parameters k is -36.006116638251136 and b is 76.97146222950178\n",
      "Iteration 312, the loss is 171.80612624497834, parameters k is -35.99983200386379 and b is 76.97246222950179\n",
      "Iteration 313, the loss is 171.7656296155955, parameters k is -35.99354736947644 and b is 76.97346222950179\n",
      "Iteration 314, the loss is 171.72513298621297, parameters k is -35.98726273508909 and b is 76.9744622295018\n",
      "Iteration 315, the loss is 171.68463635683023, parameters k is -35.98097810070174 and b is 76.9754622295018\n",
      "Iteration 316, the loss is 171.64413972744788, parameters k is -35.97469346631439 and b is 76.9764622295018\n",
      "Iteration 317, the loss is 171.603643098065, parameters k is -35.96840883192704 and b is 76.97746222950181\n",
      "Iteration 318, the loss is 171.56314646868236, parameters k is -35.962124197539694 and b is 76.97846222950182\n",
      "Iteration 319, the loss is 171.52264983929965, parameters k is -35.955839563152345 and b is 76.97946222950182\n",
      "Iteration 320, the loss is 171.48215320991696, parameters k is -35.949554928765 and b is 76.98046222950182\n",
      "Iteration 321, the loss is 171.4416565805344, parameters k is -35.94327029437765 and b is 76.98146222950183\n",
      "Iteration 322, the loss is 171.40115995115164, parameters k is -35.9369856599903 and b is 76.98246222950183\n",
      "Iteration 323, the loss is 171.36066332176895, parameters k is -35.93070102560295 and b is 76.98346222950184\n",
      "Iteration 324, the loss is 171.32016669238607, parameters k is -35.9244163912156 and b is 76.98446222950184\n",
      "Iteration 325, the loss is 171.27967006300352, parameters k is -35.91813175682825 and b is 76.98546222950185\n",
      "Iteration 326, the loss is 171.2391734336211, parameters k is -35.9118471224409 and b is 76.98646222950185\n",
      "Iteration 327, the loss is 171.19867680423815, parameters k is -35.905562488053555 and b is 76.98746222950186\n",
      "Iteration 328, the loss is 171.1581801748557, parameters k is -35.899277853666206 and b is 76.98846222950186\n",
      "Iteration 329, the loss is 171.11768354547283, parameters k is -35.89299321927886 and b is 76.98946222950187\n",
      "Iteration 330, the loss is 171.07718691609008, parameters k is -35.88670858489151 and b is 76.99046222950187\n",
      "Iteration 331, the loss is 171.03669028670788, parameters k is -35.88042395050416 and b is 76.99146222950188\n",
      "Iteration 332, the loss is 170.9961936573246, parameters k is -35.87413931611681 and b is 76.99246222950188\n",
      "Iteration 333, the loss is 170.95569702794228, parameters k is -35.86785468172946 and b is 76.99346222950189\n",
      "Iteration 334, the loss is 170.91520039855956, parameters k is -35.86157004734211 and b is 76.99446222950189\n",
      "Iteration 335, the loss is 170.87470376917693, parameters k is -35.855285412954764 and b is 76.9954622295019\n",
      "Iteration 336, the loss is 170.83420713979416, parameters k is -35.849000778567415 and b is 76.9964622295019\n",
      "Iteration 337, the loss is 170.79371051041161, parameters k is -35.842716144180066 and b is 76.9974622295019\n",
      "Iteration 338, the loss is 170.7532138810289, parameters k is -35.83643150979272 and b is 76.99846222950191\n",
      "Iteration 339, the loss is 170.712717251646, parameters k is -35.83014687540537 and b is 76.99946222950192\n",
      "Iteration 340, the loss is 170.67222062226367, parameters k is -35.82386224101802 and b is 77.00046222950192\n",
      "Iteration 341, the loss is 170.63172399288084, parameters k is -35.81757760663067 and b is 77.00146222950193\n",
      "Iteration 342, the loss is 170.59122736349826, parameters k is -35.81129297224332 and b is 77.00246222950193\n",
      "Iteration 343, the loss is 170.55073073411552, parameters k is -35.80500833785597 and b is 77.00346222950193\n",
      "Iteration 344, the loss is 170.5102341047329, parameters k is -35.798723703468625 and b is 77.00446222950194\n",
      "Iteration 345, the loss is 170.4697374753501, parameters k is -35.792439069081276 and b is 77.00546222950194\n",
      "Iteration 346, the loss is 170.4292408459673, parameters k is -35.78615443469393 and b is 77.00646222950195\n",
      "Iteration 347, the loss is 170.38874421658468, parameters k is -35.77986980030658 and b is 77.00746222950195\n",
      "Iteration 348, the loss is 170.34824758720205, parameters k is -35.77358516591923 and b is 77.00846222950196\n",
      "Iteration 349, the loss is 170.30775095781954, parameters k is -35.76730053153188 and b is 77.00946222950196\n",
      "Iteration 350, the loss is 170.26725432843676, parameters k is -35.76101589714453 and b is 77.01046222950197\n",
      "Iteration 351, the loss is 170.226757699054, parameters k is -35.75473126275718 and b is 77.01146222950197\n",
      "Iteration 352, the loss is 170.18626106967145, parameters k is -35.748446628369834 and b is 77.01246222950198\n",
      "Iteration 353, the loss is 170.14576444028893, parameters k is -35.742161993982485 and b is 77.01346222950198\n",
      "Iteration 354, the loss is 170.10526781090616, parameters k is -35.735877359595136 and b is 77.01446222950199\n",
      "Iteration 355, the loss is 170.0647711815233, parameters k is -35.72959272520779 and b is 77.01546222950199\n",
      "Iteration 356, the loss is 170.02427455214087, parameters k is -35.72330809082044 and b is 77.016462229502\n",
      "Iteration 357, the loss is 169.98377792275812, parameters k is -35.71702345643309 and b is 77.017462229502\n",
      "Iteration 358, the loss is 169.94328129337532, parameters k is -35.71073882204574 and b is 77.018462229502\n",
      "Iteration 359, the loss is 169.90278466399255, parameters k is -35.70445418765839 and b is 77.01946222950201\n",
      "Iteration 360, the loss is 169.86228803461017, parameters k is -35.69816955327104 and b is 77.02046222950202\n",
      "Iteration 361, the loss is 169.82179140522732, parameters k is -35.691884918883694 and b is 77.02146222950202\n",
      "Iteration 362, the loss is 169.78129477584474, parameters k is -35.685600284496346 and b is 77.02246222950203\n",
      "Iteration 363, the loss is 169.74079814646205, parameters k is -35.679315650109 and b is 77.02346222950203\n",
      "Iteration 364, the loss is 169.70030151707948, parameters k is -35.67303101572165 and b is 77.02446222950203\n",
      "Iteration 365, the loss is 169.65980488769694, parameters k is -35.6667463813343 and b is 77.02546222950204\n",
      "Iteration 366, the loss is 169.61930825831413, parameters k is -35.66046174694695 and b is 77.02646222950204\n",
      "Iteration 367, the loss is 169.5788116289314, parameters k is -35.6541771125596 and b is 77.02746222950205\n",
      "Iteration 368, the loss is 169.53831499954873, parameters k is -35.64789247817225 and b is 77.02846222950205\n",
      "Iteration 369, the loss is 169.49781837016616, parameters k is -35.641607843784904 and b is 77.02946222950206\n",
      "Iteration 370, the loss is 169.45732174078339, parameters k is -35.635323209397555 and b is 77.03046222950206\n",
      "Iteration 371, the loss is 169.4168251114009, parameters k is -35.629038575010206 and b is 77.03146222950207\n",
      "Iteration 372, the loss is 169.3763284820183, parameters k is -35.62275394062286 and b is 77.03246222950207\n",
      "Iteration 373, the loss is 169.33583185263532, parameters k is -35.61646930623551 and b is 77.03346222950208\n",
      "Iteration 374, the loss is 169.29533522325283, parameters k is -35.61018467184816 and b is 77.03446222950208\n",
      "Iteration 375, the loss is 169.25483859386998, parameters k is -35.60390003746081 and b is 77.03546222950209\n",
      "Iteration 376, the loss is 169.2143419644875, parameters k is -35.59761540307346 and b is 77.03646222950209\n",
      "Iteration 377, the loss is 169.17384533510483, parameters k is -35.59133076868611 and b is 77.0374622295021\n",
      "Iteration 378, the loss is 169.13334870572214, parameters k is -35.585046134298764 and b is 77.0384622295021\n",
      "Iteration 379, the loss is 169.0928520763395, parameters k is -35.578761499911415 and b is 77.0394622295021\n",
      "Iteration 380, the loss is 169.0523554469567, parameters k is -35.57247686552407 and b is 77.04046222950211\n",
      "Iteration 381, the loss is 169.01185881757408, parameters k is -35.56619223113672 and b is 77.04146222950212\n",
      "Iteration 382, the loss is 168.97136218819128, parameters k is -35.55990759674937 and b is 77.04246222950212\n",
      "Iteration 383, the loss is 168.93086555880873, parameters k is -35.55362296236202 and b is 77.04346222950213\n",
      "Iteration 384, the loss is 168.89036892942593, parameters k is -35.54733832797467 and b is 77.04446222950213\n",
      "Iteration 385, the loss is 168.8498723000434, parameters k is -35.54105369358732 and b is 77.04546222950214\n",
      "Iteration 386, the loss is 168.80937567066061, parameters k is -35.534769059199974 and b is 77.04646222950214\n",
      "Iteration 387, the loss is 168.7688790412781, parameters k is -35.528484424812625 and b is 77.04746222950214\n",
      "Iteration 388, the loss is 168.7283824118952, parameters k is -35.522199790425276 and b is 77.04846222950215\n",
      "Iteration 389, the loss is 168.6878857825126, parameters k is -35.51591515603793 and b is 77.04946222950215\n",
      "Iteration 390, the loss is 168.64738915312992, parameters k is -35.50963052165058 and b is 77.05046222950216\n",
      "Iteration 391, the loss is 168.60689252374735, parameters k is -35.50334588726323 and b is 77.05146222950216\n",
      "Iteration 392, the loss is 168.5663958943649, parameters k is -35.49706125287588 and b is 77.05246222950217\n",
      "Iteration 393, the loss is 168.525899264982, parameters k is -35.49077661848853 and b is 77.05346222950217\n",
      "Iteration 394, the loss is 168.48540263559943, parameters k is -35.48449198410118 and b is 77.05446222950218\n",
      "Iteration 395, the loss is 168.44490600621677, parameters k is -35.478207349713834 and b is 77.05546222950218\n",
      "Iteration 396, the loss is 168.40440937683394, parameters k is -35.471922715326485 and b is 77.05646222950219\n",
      "Iteration 397, the loss is 168.3639127474513, parameters k is -35.46563808093914 and b is 77.05746222950219\n",
      "Iteration 398, the loss is 168.32341611806885, parameters k is -35.45935344655179 and b is 77.0584622295022\n",
      "Iteration 399, the loss is 168.2829194886858, parameters k is -35.45306881216444 and b is 77.0594622295022\n",
      "Iteration 400, the loss is 168.24242285930325, parameters k is -35.44678417777709 and b is 77.0604622295022\n",
      "Iteration 401, the loss is 168.20192622992067, parameters k is -35.44049954338974 and b is 77.06146222950221\n",
      "Iteration 402, the loss is 168.16142960053773, parameters k is -35.43421490900239 and b is 77.06246222950222\n",
      "Iteration 403, the loss is 168.12093297115533, parameters k is -35.42793027461504 and b is 77.06346222950222\n",
      "Iteration 404, the loss is 168.0804363417725, parameters k is -35.421645640227695 and b is 77.06446222950223\n",
      "Iteration 405, the loss is 168.0399397123901, parameters k is -35.415361005840346 and b is 77.06546222950223\n",
      "Iteration 406, the loss is 167.9994430830073, parameters k is -35.409076371453 and b is 77.06646222950224\n",
      "Iteration 407, the loss is 167.95894645362463, parameters k is -35.40279173706565 and b is 77.06746222950224\n",
      "Iteration 408, the loss is 167.91844982424212, parameters k is -35.3965071026783 and b is 77.06846222950225\n",
      "Iteration 409, the loss is 167.87795319485937, parameters k is -35.39022246829095 and b is 77.06946222950225\n",
      "Iteration 410, the loss is 167.83745656547657, parameters k is -35.3839378339036 and b is 77.07046222950225\n",
      "Iteration 411, the loss is 167.79695993609403, parameters k is -35.37765319951625 and b is 77.07146222950226\n",
      "Iteration 412, the loss is 167.75646330671117, parameters k is -35.371368565128904 and b is 77.07246222950226\n",
      "Iteration 413, the loss is 167.7159666773286, parameters k is -35.365083930741555 and b is 77.07346222950227\n",
      "Iteration 414, the loss is 167.6754700479457, parameters k is -35.358799296354206 and b is 77.07446222950227\n",
      "Iteration 415, the loss is 167.63497341856313, parameters k is -35.35251466196686 and b is 77.07546222950228\n",
      "Iteration 416, the loss is 167.59447678918062, parameters k is -35.34623002757951 and b is 77.07646222950228\n",
      "Iteration 417, the loss is 167.55398015979785, parameters k is -35.33994539319216 and b is 77.07746222950229\n",
      "Iteration 418, the loss is 167.51348353041507, parameters k is -35.33366075880481 and b is 77.07846222950229\n",
      "Iteration 419, the loss is 167.47298690103253, parameters k is -35.32737612441746 and b is 77.0794622295023\n",
      "Iteration 420, the loss is 167.43249027164967, parameters k is -35.32109149003011 and b is 77.0804622295023\n",
      "Iteration 421, the loss is 167.39199364226715, parameters k is -35.314806855642765 and b is 77.0814622295023\n",
      "Iteration 422, the loss is 167.35149701288452, parameters k is -35.308522221255416 and b is 77.08246222950231\n",
      "Iteration 423, the loss is 167.31100038350192, parameters k is -35.30223758686807 and b is 77.08346222950232\n",
      "Iteration 424, the loss is 167.27050375411915, parameters k is -35.29595295248072 and b is 77.08446222950232\n",
      "Iteration 425, the loss is 167.23000712473674, parameters k is -35.28966831809337 and b is 77.08546222950233\n",
      "Iteration 426, the loss is 167.18951049535383, parameters k is -35.28338368370602 and b is 77.08646222950233\n",
      "Iteration 427, the loss is 167.14901386597126, parameters k is -35.27709904931867 and b is 77.08746222950234\n",
      "Iteration 428, the loss is 167.10851723658863, parameters k is -35.27081441493132 and b is 77.08846222950234\n",
      "Iteration 429, the loss is 167.06802060720574, parameters k is -35.264529780543974 and b is 77.08946222950235\n",
      "Iteration 430, the loss is 167.02752397782322, parameters k is -35.258245146156625 and b is 77.09046222950235\n",
      "Iteration 431, the loss is 166.9870273484404, parameters k is -35.251960511769276 and b is 77.09146222950235\n",
      "Iteration 432, the loss is 166.94653071905796, parameters k is -35.24567587738193 and b is 77.09246222950236\n",
      "Iteration 433, the loss is 166.90603408967496, parameters k is -35.23939124299458 and b is 77.09346222950236\n",
      "Iteration 434, the loss is 166.86553746029244, parameters k is -35.23310660860723 and b is 77.09446222950237\n",
      "Iteration 435, the loss is 166.82504083090993, parameters k is -35.22682197421988 and b is 77.09546222950237\n",
      "Iteration 436, the loss is 166.7845442015272, parameters k is -35.22053733983253 and b is 77.09646222950238\n",
      "Iteration 437, the loss is 166.7440475721445, parameters k is -35.21425270544518 and b is 77.09746222950238\n",
      "Iteration 438, the loss is 166.70355094276184, parameters k is -35.207968071057834 and b is 77.09846222950239\n",
      "Iteration 439, the loss is 166.66305431337923, parameters k is -35.201683436670486 and b is 77.0994622295024\n",
      "Iteration 440, the loss is 166.62255768399638, parameters k is -35.19539880228314 and b is 77.1004622295024\n",
      "Iteration 441, the loss is 166.5820610546141, parameters k is -35.18911416789579 and b is 77.1014622295024\n",
      "Iteration 442, the loss is 166.54156442523106, parameters k is -35.18282953350844 and b is 77.10246222950241\n",
      "Iteration 443, the loss is 166.50106779584857, parameters k is -35.17654489912109 and b is 77.10346222950241\n",
      "Iteration 444, the loss is 166.4605711664658, parameters k is -35.17026026473374 and b is 77.10446222950242\n",
      "Iteration 445, the loss is 166.42007453708334, parameters k is -35.16397563034639 and b is 77.10546222950242\n",
      "Iteration 446, the loss is 166.3795779077004, parameters k is -35.157690995959044 and b is 77.10646222950243\n",
      "Iteration 447, the loss is 166.3390812783178, parameters k is -35.151406361571695 and b is 77.10746222950243\n",
      "Iteration 448, the loss is 166.29858464893485, parameters k is -35.145121727184346 and b is 77.10846222950244\n",
      "Iteration 449, the loss is 166.25808801955247, parameters k is -35.138837092797 and b is 77.10946222950244\n",
      "Iteration 450, the loss is 166.21759139016984, parameters k is -35.13255245840965 and b is 77.11046222950245\n",
      "Iteration 451, the loss is 166.17709476078704, parameters k is -35.1262678240223 and b is 77.11146222950245\n",
      "Iteration 452, the loss is 166.13659813140438, parameters k is -35.11998318963495 and b is 77.11246222950246\n",
      "Iteration 453, the loss is 166.09610150202178, parameters k is -35.1136985552476 and b is 77.11346222950246\n",
      "Iteration 454, the loss is 166.05560487263904, parameters k is -35.10741392086025 and b is 77.11446222950246\n",
      "Iteration 455, the loss is 166.01510824325652, parameters k is -35.101129286472904 and b is 77.11546222950247\n",
      "Iteration 456, the loss is 165.97461161387378, parameters k is -35.094844652085555 and b is 77.11646222950247\n",
      "Iteration 457, the loss is 165.93411498449115, parameters k is -35.08856001769821 and b is 77.11746222950248\n",
      "Iteration 458, the loss is 165.89361835510832, parameters k is -35.08227538331086 and b is 77.11846222950248\n",
      "Iteration 459, the loss is 165.8531217257258, parameters k is -35.07599074892351 and b is 77.11946222950249\n",
      "Iteration 460, the loss is 165.81262509634303, parameters k is -35.06970611453616 and b is 77.1204622295025\n",
      "Iteration 461, the loss is 165.77212846696045, parameters k is -35.06342148014881 and b is 77.1214622295025\n",
      "Iteration 462, the loss is 165.73163183757762, parameters k is -35.05713684576146 and b is 77.1224622295025\n",
      "Iteration 463, the loss is 165.6911352081951, parameters k is -35.050852211374114 and b is 77.12346222950251\n",
      "Iteration 464, the loss is 165.65063857881236, parameters k is -35.044567576986765 and b is 77.12446222950251\n",
      "Iteration 465, the loss is 165.61014194942967, parameters k is -35.038282942599416 and b is 77.12546222950252\n",
      "Iteration 466, the loss is 165.56964532004696, parameters k is -35.03199830821207 and b is 77.12646222950252\n",
      "Iteration 467, the loss is 165.5291486906643, parameters k is -35.02571367382472 and b is 77.12746222950253\n",
      "Iteration 468, the loss is 165.4886520612817, parameters k is -35.01942903943737 and b is 77.12846222950253\n",
      "Iteration 469, the loss is 165.44815543189907, parameters k is -35.01314440505002 and b is 77.12946222950254\n",
      "Iteration 470, the loss is 165.4076588025163, parameters k is -35.00685977066267 and b is 77.13046222950254\n",
      "Iteration 471, the loss is 165.3671621731337, parameters k is -35.00057513627532 and b is 77.13146222950255\n",
      "Iteration 472, the loss is 165.32666554375086, parameters k is -34.994290501887974 and b is 77.13246222950255\n",
      "Iteration 473, the loss is 165.28616891436803, parameters k is -34.988005867500625 and b is 77.13346222950256\n",
      "Iteration 474, the loss is 165.24567228498557, parameters k is -34.98172123311328 and b is 77.13446222950256\n",
      "Iteration 475, the loss is 165.20517565560294, parameters k is -34.97543659872593 and b is 77.13546222950256\n",
      "Iteration 476, the loss is 165.16467902622037, parameters k is -34.96915196433858 and b is 77.13646222950257\n",
      "Iteration 477, the loss is 165.12418239683765, parameters k is -34.96286732995123 and b is 77.13746222950257\n",
      "Iteration 478, the loss is 165.0836857674548, parameters k is -34.95658269556388 and b is 77.13846222950258\n",
      "Iteration 479, the loss is 165.04318913807228, parameters k is -34.95029806117653 and b is 77.13946222950258\n",
      "Iteration 480, the loss is 165.0026925086897, parameters k is -34.94401342678918 and b is 77.14046222950259\n",
      "Iteration 481, the loss is 164.96219587930693, parameters k is -34.937728792401835 and b is 77.1414622295026\n",
      "Iteration 482, the loss is 164.92169924992427, parameters k is -34.931444158014486 and b is 77.1424622295026\n",
      "Iteration 483, the loss is 164.88120262054161, parameters k is -34.92515952362714 and b is 77.1434622295026\n",
      "Iteration 484, the loss is 164.8407059911588, parameters k is -34.91887488923979 and b is 77.14446222950261\n",
      "Iteration 485, the loss is 164.80020936177613, parameters k is -34.91259025485244 and b is 77.14546222950261\n",
      "Iteration 486, the loss is 164.75971273239367, parameters k is -34.90630562046509 and b is 77.14646222950262\n",
      "Iteration 487, the loss is 164.7192161030109, parameters k is -34.90002098607774 and b is 77.14746222950262\n",
      "Iteration 488, the loss is 164.67871947362815, parameters k is -34.89373635169039 and b is 77.14846222950263\n",
      "Iteration 489, the loss is 164.63822284424566, parameters k is -34.887451717303044 and b is 77.14946222950263\n",
      "Iteration 490, the loss is 164.59772621486292, parameters k is -34.881167082915695 and b is 77.15046222950264\n",
      "Iteration 491, the loss is 164.55722958548017, parameters k is -34.874882448528346 and b is 77.15146222950264\n",
      "Iteration 492, the loss is 164.51673295609774, parameters k is -34.868597814141 and b is 77.15246222950265\n",
      "Iteration 493, the loss is 164.4762363267147, parameters k is -34.86231317975365 and b is 77.15346222950265\n",
      "Iteration 494, the loss is 164.43573969733228, parameters k is -34.8560285453663 and b is 77.15446222950266\n",
      "Iteration 495, the loss is 164.3952430679495, parameters k is -34.84974391097895 and b is 77.15546222950266\n",
      "Iteration 496, the loss is 164.35474643856696, parameters k is -34.8434592765916 and b is 77.15646222950267\n",
      "Iteration 497, the loss is 164.31424980918436, parameters k is -34.83717464220425 and b is 77.15746222950267\n",
      "Iteration 498, the loss is 164.27375317980156, parameters k is -34.830890007816905 and b is 77.15846222950267\n",
      "Iteration 499, the loss is 164.2332565504189, parameters k is -34.824605373429556 and b is 77.15946222950268\n",
      "Iteration 500, the loss is 164.1927599210362, parameters k is -34.81832073904221 and b is 77.16046222950268\n",
      "Iteration 501, the loss is 164.1522632916535, parameters k is -34.81203610465486 and b is 77.16146222950269\n",
      "Iteration 502, the loss is 164.11176666227087, parameters k is -34.80575147026751 and b is 77.1624622295027\n",
      "Iteration 503, the loss is 164.07127003288818, parameters k is -34.79946683588016 and b is 77.1634622295027\n",
      "Iteration 504, the loss is 164.03077340350546, parameters k is -34.79318220149281 and b is 77.1644622295027\n",
      "Iteration 505, the loss is 163.99027677412283, parameters k is -34.78689756710546 and b is 77.16546222950271\n",
      "Iteration 506, the loss is 163.9497801447402, parameters k is -34.780612932718114 and b is 77.16646222950271\n",
      "Iteration 507, the loss is 163.90928351535769, parameters k is -34.774328298330765 and b is 77.16746222950272\n",
      "Iteration 508, the loss is 163.8687868859746, parameters k is -34.768043663943416 and b is 77.16846222950272\n",
      "Iteration 509, the loss is 163.82829025659225, parameters k is -34.76175902955607 and b is 77.16946222950273\n",
      "Iteration 510, the loss is 163.78779362720965, parameters k is -34.75547439516872 and b is 77.17046222950273\n",
      "Iteration 511, the loss is 163.74729699782685, parameters k is -34.74918976078137 and b is 77.17146222950274\n",
      "Iteration 512, the loss is 163.70680036844416, parameters k is -34.74290512639402 and b is 77.17246222950274\n",
      "Iteration 513, the loss is 163.66630373906145, parameters k is -34.73662049200667 and b is 77.17346222950275\n",
      "Iteration 514, the loss is 163.62580710967885, parameters k is -34.73033585761932 and b is 77.17446222950275\n",
      "Iteration 515, the loss is 163.5853104802962, parameters k is -34.724051223231974 and b is 77.17546222950276\n",
      "Iteration 516, the loss is 163.54481385091353, parameters k is -34.717766588844626 and b is 77.17646222950276\n",
      "Iteration 517, the loss is 163.5043172215307, parameters k is -34.71148195445728 and b is 77.17746222950277\n",
      "Iteration 518, the loss is 163.46382059214827, parameters k is -34.70519732006993 and b is 77.17846222950277\n",
      "Iteration 519, the loss is 163.42332396276552, parameters k is -34.69891268568258 and b is 77.17946222950278\n",
      "Iteration 520, the loss is 163.38282733338275, parameters k is -34.69262805129523 and b is 77.18046222950278\n",
      "Iteration 521, the loss is 163.3423307040001, parameters k is -34.68634341690788 and b is 77.18146222950278\n",
      "Iteration 522, the loss is 163.30183407461723, parameters k is -34.68005878252053 and b is 77.18246222950279\n",
      "Iteration 523, the loss is 163.2613374452348, parameters k is -34.673774148133184 and b is 77.1834622295028\n",
      "Iteration 524, the loss is 163.22084081585209, parameters k is -34.667489513745835 and b is 77.1844622295028\n",
      "Iteration 525, the loss is 163.1803441864696, parameters k is -34.661204879358486 and b is 77.1854622295028\n",
      "Iteration 526, the loss is 163.1398475570869, parameters k is -34.65492024497114 and b is 77.18646222950281\n",
      "Iteration 527, the loss is 163.09935092770417, parameters k is -34.64863561058379 and b is 77.18746222950281\n",
      "Iteration 528, the loss is 163.0588542983215, parameters k is -34.64235097619644 and b is 77.18846222950282\n",
      "Iteration 529, the loss is 163.01835766893876, parameters k is -34.63606634180909 and b is 77.18946222950282\n",
      "Iteration 530, the loss is 162.9778610395561, parameters k is -34.62978170742174 and b is 77.19046222950283\n",
      "Iteration 531, the loss is 162.93736441017347, parameters k is -34.62349707303439 and b is 77.19146222950283\n",
      "Iteration 532, the loss is 162.89686778079067, parameters k is -34.617212438647044 and b is 77.19246222950284\n",
      "Iteration 533, the loss is 162.8563711514079, parameters k is -34.610927804259696 and b is 77.19346222950284\n",
      "Iteration 534, the loss is 162.81587452202533, parameters k is -34.60464316987235 and b is 77.19446222950285\n",
      "Iteration 535, the loss is 162.77537789264258, parameters k is -34.598358535485 and b is 77.19546222950285\n",
      "Iteration 536, the loss is 162.73488126325995, parameters k is -34.59207390109765 and b is 77.19646222950286\n",
      "Iteration 537, the loss is 162.69438463387723, parameters k is -34.5857892667103 and b is 77.19746222950286\n",
      "Iteration 538, the loss is 162.65388800449466, parameters k is -34.57950463232295 and b is 77.19846222950287\n",
      "Iteration 539, the loss is 162.61339137511223, parameters k is -34.5732199979356 and b is 77.19946222950287\n",
      "Iteration 540, the loss is 162.57289474572946, parameters k is -34.566935363548254 and b is 77.20046222950288\n",
      "Iteration 541, the loss is 162.53239811634674, parameters k is -34.560650729160905 and b is 77.20146222950288\n",
      "Iteration 542, the loss is 162.49190148696408, parameters k is -34.554366094773556 and b is 77.20246222950288\n",
      "Iteration 543, the loss is 162.45140485758148, parameters k is -34.54808146038621 and b is 77.20346222950289\n",
      "Iteration 544, the loss is 162.4109082281987, parameters k is -34.54179682599886 and b is 77.2044622295029\n",
      "Iteration 545, the loss is 162.37041159881588, parameters k is -34.53551219161151 and b is 77.2054622295029\n",
      "Iteration 546, the loss is 162.32991496943333, parameters k is -34.52922755722416 and b is 77.2064622295029\n",
      "Iteration 547, the loss is 162.28941834005073, parameters k is -34.52294292283681 and b is 77.20746222950291\n",
      "Iteration 548, the loss is 162.24892171066801, parameters k is -34.51665828844946 and b is 77.20846222950291\n",
      "Iteration 549, the loss is 162.2084250812854, parameters k is -34.510373654062114 and b is 77.20946222950292\n",
      "Iteration 550, the loss is 162.1679284519028, parameters k is -34.504089019674765 and b is 77.21046222950292\n",
      "Iteration 551, the loss is 162.1274318225198, parameters k is -34.49780438528742 and b is 77.21146222950293\n",
      "Iteration 552, the loss is 162.0869351931374, parameters k is -34.49151975090007 and b is 77.21246222950293\n",
      "Iteration 553, the loss is 162.04643856375463, parameters k is -34.48523511651272 and b is 77.21346222950294\n",
      "Iteration 554, the loss is 162.00594193437198, parameters k is -34.47895048212537 and b is 77.21446222950294\n",
      "Iteration 555, the loss is 161.96544530498923, parameters k is -34.47266584773802 and b is 77.21546222950295\n",
      "Iteration 556, the loss is 161.92494867560666, parameters k is -34.46638121335067 and b is 77.21646222950295\n",
      "Iteration 557, the loss is 161.88445204622394, parameters k is -34.460096578963324 and b is 77.21746222950296\n",
      "Iteration 558, the loss is 161.84395541684154, parameters k is -34.453811944575975 and b is 77.21846222950296\n",
      "Iteration 559, the loss is 161.80345878745874, parameters k is -34.447527310188626 and b is 77.21946222950297\n",
      "Iteration 560, the loss is 161.76296215807605, parameters k is -34.44124267580128 and b is 77.22046222950297\n",
      "Iteration 561, the loss is 161.72246552869322, parameters k is -34.43495804141393 and b is 77.22146222950298\n",
      "Iteration 562, the loss is 161.6819688993104, parameters k is -34.42867340702658 and b is 77.22246222950298\n",
      "Iteration 563, the loss is 161.64147226992793, parameters k is -34.42238877263923 and b is 77.22346222950299\n",
      "Iteration 564, the loss is 161.60097564054527, parameters k is -34.41610413825188 and b is 77.22446222950299\n",
      "Iteration 565, the loss is 161.56047901116267, parameters k is -34.40981950386453 and b is 77.225462229503\n",
      "Iteration 566, the loss is 161.51998238177998, parameters k is -34.403534869477184 and b is 77.226462229503\n",
      "Iteration 567, the loss is 161.47948575239738, parameters k is -34.397250235089835 and b is 77.227462229503\n",
      "Iteration 568, the loss is 161.43898912301484, parameters k is -34.390965600702486 and b is 77.22846222950301\n",
      "Iteration 569, the loss is 161.39849249363206, parameters k is -34.38468096631514 and b is 77.22946222950301\n",
      "Iteration 570, the loss is 161.3579958642493, parameters k is -34.37839633192779 and b is 77.23046222950302\n",
      "Iteration 571, the loss is 161.31749923486655, parameters k is -34.37211169754044 and b is 77.23146222950302\n",
      "Iteration 572, the loss is 161.27700260548403, parameters k is -34.36582706315309 and b is 77.23246222950303\n",
      "Iteration 573, the loss is 161.23650597610103, parameters k is -34.35954242876574 and b is 77.23346222950303\n",
      "Iteration 574, the loss is 161.19600934671854, parameters k is -34.35325779437839 and b is 77.23446222950304\n",
      "Iteration 575, the loss is 161.15551271733605, parameters k is -34.346973159991045 and b is 77.23546222950304\n",
      "Iteration 576, the loss is 161.11501608795328, parameters k is -34.340688525603696 and b is 77.23646222950305\n",
      "Iteration 577, the loss is 161.07451945857062, parameters k is -34.33440389121635 and b is 77.23746222950305\n",
      "Iteration 578, the loss is 161.03402282918793, parameters k is -34.328119256829 and b is 77.23846222950306\n",
      "Iteration 579, the loss is 160.9935261998054, parameters k is -34.32183462244165 and b is 77.23946222950306\n",
      "Iteration 580, the loss is 160.95302957042253, parameters k is -34.3155499880543 and b is 77.24046222950307\n",
      "Iteration 581, the loss is 160.91253294103984, parameters k is -34.30926535366695 and b is 77.24146222950307\n",
      "Iteration 582, the loss is 160.8720363116572, parameters k is -34.3029807192796 and b is 77.24246222950308\n",
      "Iteration 583, the loss is 160.8315396822746, parameters k is -34.296696084892254 and b is 77.24346222950308\n",
      "Iteration 584, the loss is 160.79104305289192, parameters k is -34.290411450504905 and b is 77.24446222950309\n",
      "Iteration 585, the loss is 160.7505464235092, parameters k is -34.284126816117556 and b is 77.24546222950309\n",
      "Iteration 586, the loss is 160.7100497941266, parameters k is -34.27784218173021 and b is 77.2464622295031\n",
      "Iteration 587, the loss is 160.669553164744, parameters k is -34.27155754734286 and b is 77.2474622295031\n",
      "Iteration 588, the loss is 160.62905653536117, parameters k is -34.26527291295551 and b is 77.2484622295031\n",
      "Iteration 589, the loss is 160.58855990597837, parameters k is -34.25898827856816 and b is 77.24946222950311\n",
      "Iteration 590, the loss is 160.54806327659594, parameters k is -34.25270364418081 and b is 77.25046222950311\n",
      "Iteration 591, the loss is 160.5075666472132, parameters k is -34.24641900979346 and b is 77.25146222950312\n",
      "Iteration 592, the loss is 160.46707001783068, parameters k is -34.240134375406114 and b is 77.25246222950312\n",
      "Iteration 593, the loss is 160.42657338844782, parameters k is -34.233849741018766 and b is 77.25346222950313\n",
      "Iteration 594, the loss is 160.38607675906528, parameters k is -34.22756510663142 and b is 77.25446222950313\n",
      "Iteration 595, the loss is 160.34558012968236, parameters k is -34.22128047224407 and b is 77.25546222950314\n",
      "Iteration 596, the loss is 160.30508350029973, parameters k is -34.21499583785672 and b is 77.25646222950314\n",
      "Iteration 597, the loss is 160.26458687091716, parameters k is -34.20871120346937 and b is 77.25746222950315\n",
      "Iteration 598, the loss is 160.22409024153444, parameters k is -34.20242656908202 and b is 77.25846222950315\n",
      "Iteration 599, the loss is 160.18359361215175, parameters k is -34.19614193469467 and b is 77.25946222950316\n",
      "Iteration 600, the loss is 160.14309698276915, parameters k is -34.189857300307324 and b is 77.26046222950316\n",
      "Iteration 601, the loss is 160.10260035338655, parameters k is -34.183572665919975 and b is 77.26146222950317\n",
      "Iteration 602, the loss is 160.06210372400383, parameters k is -34.177288031532626 and b is 77.26246222950317\n",
      "Iteration 603, the loss is 160.02160709462132, parameters k is -34.17100339714528 and b is 77.26346222950318\n",
      "Iteration 604, the loss is 159.98111046523846, parameters k is -34.16471876275793 and b is 77.26446222950318\n",
      "Iteration 605, the loss is 159.94061383585586, parameters k is -34.15843412837058 and b is 77.26546222950319\n",
      "Iteration 606, the loss is 159.9001172064732, parameters k is -34.15214949398323 and b is 77.26646222950319\n",
      "Iteration 607, the loss is 159.85962057709028, parameters k is -34.14586485959588 and b is 77.2674622295032\n",
      "Iteration 608, the loss is 159.81912394770782, parameters k is -34.13958022520853 and b is 77.2684622295032\n",
      "Iteration 609, the loss is 159.77862731832508, parameters k is -34.133295590821184 and b is 77.2694622295032\n",
      "Iteration 610, the loss is 159.7381306889423, parameters k is -34.127010956433836 and b is 77.27046222950321\n",
      "Iteration 611, the loss is 159.69763405955976, parameters k is -34.12072632204649 and b is 77.27146222950321\n",
      "Iteration 612, the loss is 159.65713743017722, parameters k is -34.11444168765914 and b is 77.27246222950322\n",
      "Iteration 613, the loss is 159.61664080079444, parameters k is -34.10815705327179 and b is 77.27346222950322\n",
      "Iteration 614, the loss is 159.57614417141178, parameters k is -34.10187241888444 and b is 77.27446222950323\n",
      "Iteration 615, the loss is 159.5356475420293, parameters k is -34.09558778449709 and b is 77.27546222950323\n",
      "Iteration 616, the loss is 159.4951509126466, parameters k is -34.08930315010974 and b is 77.27646222950324\n",
      "Iteration 617, the loss is 159.45465428326366, parameters k is -34.083018515722394 and b is 77.27746222950324\n",
      "Iteration 618, the loss is 159.41415765388112, parameters k is -34.076733881335045 and b is 77.27846222950325\n",
      "Iteration 619, the loss is 159.37366102449846, parameters k is -34.070449246947696 and b is 77.27946222950325\n",
      "Iteration 620, the loss is 159.3331643951156, parameters k is -34.06416461256035 and b is 77.28046222950326\n",
      "Iteration 621, the loss is 159.29266776573274, parameters k is -34.057879978173 and b is 77.28146222950326\n",
      "Iteration 622, the loss is 159.25217113635028, parameters k is -34.05159534378565 and b is 77.28246222950327\n",
      "Iteration 623, the loss is 159.2116745069677, parameters k is -34.0453107093983 and b is 77.28346222950327\n",
      "Iteration 624, the loss is 159.17117787758514, parameters k is -34.03902607501095 and b is 77.28446222950328\n",
      "Iteration 625, the loss is 159.13068124820234, parameters k is -34.0327414406236 and b is 77.28546222950328\n",
      "Iteration 626, the loss is 159.09018461881985, parameters k is -34.026456806236254 and b is 77.28646222950329\n",
      "Iteration 627, the loss is 159.04968798943668, parameters k is -34.020172171848905 and b is 77.28746222950329\n",
      "Iteration 628, the loss is 159.0091913600543, parameters k is -34.01388753746156 and b is 77.2884622295033\n",
      "Iteration 629, the loss is 158.96869473067167, parameters k is -34.00760290307421 and b is 77.2894622295033\n",
      "Iteration 630, the loss is 158.92819810128907, parameters k is -34.00131826868686 and b is 77.2904622295033\n",
      "Iteration 631, the loss is 158.88770147190652, parameters k is -33.99503363429951 and b is 77.29146222950331\n",
      "Iteration 632, the loss is 158.8472048425237, parameters k is -33.98874899991216 and b is 77.29246222950331\n",
      "Iteration 633, the loss is 158.8067082131411, parameters k is -33.98246436552481 and b is 77.29346222950332\n",
      "Iteration 634, the loss is 158.76621158375846, parameters k is -33.976179731137464 and b is 77.29446222950332\n",
      "Iteration 635, the loss is 158.72571495437566, parameters k is -33.969895096750115 and b is 77.29546222950333\n",
      "Iteration 636, the loss is 158.68521832499295, parameters k is -33.963610462362766 and b is 77.29646222950333\n",
      "Iteration 637, the loss is 158.64472169561031, parameters k is -33.95732582797542 and b is 77.29746222950334\n",
      "Iteration 638, the loss is 158.6042250662278, parameters k is -33.95104119358807 and b is 77.29846222950334\n",
      "Iteration 639, the loss is 158.5637284368449, parameters k is -33.94475655920072 and b is 77.29946222950335\n",
      "Iteration 640, the loss is 158.52323180746222, parameters k is -33.93847192481337 and b is 77.30046222950335\n",
      "Iteration 641, the loss is 158.4827351780798, parameters k is -33.93218729042602 and b is 77.30146222950336\n",
      "Iteration 642, the loss is 158.44223854869713, parameters k is -33.92590265603867 and b is 77.30246222950336\n",
      "Iteration 643, the loss is 158.4017419193144, parameters k is -33.919618021651324 and b is 77.30346222950337\n",
      "Iteration 644, the loss is 158.36124528993167, parameters k is -33.913333387263975 and b is 77.30446222950337\n",
      "Iteration 645, the loss is 158.32074866054884, parameters k is -33.90704875287663 and b is 77.30546222950338\n",
      "Iteration 646, the loss is 158.2802520311664, parameters k is -33.90076411848928 and b is 77.30646222950338\n",
      "Iteration 647, the loss is 158.23975540178367, parameters k is -33.89447948410193 and b is 77.30746222950339\n",
      "Iteration 648, the loss is 158.19925877240084, parameters k is -33.88819484971458 and b is 77.30846222950339\n",
      "Iteration 649, the loss is 158.15876214301824, parameters k is -33.88191021532723 and b is 77.3094622295034\n",
      "Iteration 650, the loss is 158.11826551363566, parameters k is -33.87562558093988 and b is 77.3104622295034\n",
      "Iteration 651, the loss is 158.07776888425298, parameters k is -33.86934094655253 and b is 77.3114622295034\n",
      "Iteration 652, the loss is 158.03727225487017, parameters k is -33.863056312165185 and b is 77.31246222950341\n",
      "Iteration 653, the loss is 157.99677562548763, parameters k is -33.856771677777836 and b is 77.31346222950341\n",
      "Iteration 654, the loss is 157.9562789961051, parameters k is -33.85048704339049 and b is 77.31446222950342\n",
      "Iteration 655, the loss is 157.915782366722, parameters k is -33.84420240900314 and b is 77.31546222950342\n",
      "Iteration 656, the loss is 157.87528573733965, parameters k is -33.83791777461579 and b is 77.31646222950343\n",
      "Iteration 657, the loss is 157.83478910795685, parameters k is -33.83163314022844 and b is 77.31746222950343\n",
      "Iteration 658, the loss is 157.79429247857422, parameters k is -33.82534850584109 and b is 77.31846222950344\n",
      "Iteration 659, the loss is 157.75379584919162, parameters k is -33.81906387145374 and b is 77.31946222950344\n",
      "Iteration 660, the loss is 157.71329921980885, parameters k is -33.812779237066394 and b is 77.32046222950345\n",
      "Iteration 661, the loss is 157.67280259042607, parameters k is -33.806494602679045 and b is 77.32146222950345\n",
      "Iteration 662, the loss is 157.63230596104367, parameters k is -33.800209968291696 and b is 77.32246222950346\n",
      "Iteration 663, the loss is 157.5918093316609, parameters k is -33.79392533390435 and b is 77.32346222950346\n",
      "Iteration 664, the loss is 157.5513127022782, parameters k is -33.787640699517 and b is 77.32446222950347\n",
      "Iteration 665, the loss is 157.51081607289564, parameters k is -33.78135606512965 and b is 77.32546222950347\n",
      "Iteration 666, the loss is 157.47031944351295, parameters k is -33.7750714307423 and b is 77.32646222950348\n",
      "Iteration 667, the loss is 157.4298228141304, parameters k is -33.76878679635495 and b is 77.32746222950348\n",
      "Iteration 668, the loss is 157.3893261847475, parameters k is -33.7625021619676 and b is 77.32846222950349\n",
      "Iteration 669, the loss is 157.34882955536517, parameters k is -33.756217527580255 and b is 77.32946222950349\n",
      "Iteration 670, the loss is 157.30833292598243, parameters k is -33.749932893192906 and b is 77.3304622295035\n",
      "Iteration 671, the loss is 157.26783629659954, parameters k is -33.74364825880556 and b is 77.3314622295035\n",
      "Iteration 672, the loss is 157.227339667217, parameters k is -33.73736362441821 and b is 77.3324622295035\n",
      "Iteration 673, the loss is 157.1868430378343, parameters k is -33.73107899003086 and b is 77.33346222950351\n",
      "Iteration 674, the loss is 157.14634640845173, parameters k is -33.72479435564351 and b is 77.33446222950352\n",
      "Iteration 675, the loss is 157.1058497790688, parameters k is -33.71850972125616 and b is 77.33546222950352\n",
      "Iteration 676, the loss is 157.06535314968593, parameters k is -33.71222508686881 and b is 77.33646222950352\n",
      "Iteration 677, the loss is 157.02485652030333, parameters k is -33.705940452481464 and b is 77.33746222950353\n",
      "Iteration 678, the loss is 156.98435989092079, parameters k is -33.699655818094115 and b is 77.33846222950353\n",
      "Iteration 679, the loss is 156.94386326153813, parameters k is -33.693371183706766 and b is 77.33946222950354\n",
      "Iteration 680, the loss is 156.9033666321554, parameters k is -33.68708654931942 and b is 77.34046222950354\n",
      "Iteration 681, the loss is 156.86287000277278, parameters k is -33.68080191493207 and b is 77.34146222950355\n",
      "Iteration 682, the loss is 156.8223733733901, parameters k is -33.67451728054472 and b is 77.34246222950355\n",
      "Iteration 683, the loss is 156.78187674400772, parameters k is -33.66823264615737 and b is 77.34346222950356\n",
      "Iteration 684, the loss is 156.74138011462458, parameters k is -33.66194801177002 and b is 77.34446222950356\n",
      "Iteration 685, the loss is 156.70088348524212, parameters k is -33.65566337738267 and b is 77.34546222950357\n",
      "Iteration 686, the loss is 156.66038685585949, parameters k is -33.649378742995324 and b is 77.34646222950357\n",
      "Iteration 687, the loss is 156.6198902264769, parameters k is -33.643094108607976 and b is 77.34746222950358\n",
      "Iteration 688, the loss is 156.57939359709417, parameters k is -33.63680947422063 and b is 77.34846222950358\n",
      "Iteration 689, the loss is 156.5388969677116, parameters k is -33.63052483983328 and b is 77.34946222950359\n",
      "Iteration 690, the loss is 156.49840033832882, parameters k is -33.62424020544593 and b is 77.35046222950359\n",
      "Iteration 691, the loss is 156.45790370894622, parameters k is -33.61795557105858 and b is 77.3514622295036\n",
      "Iteration 692, the loss is 156.41740707956367, parameters k is -33.61167093667123 and b is 77.3524622295036\n",
      "Iteration 693, the loss is 156.37691045018082, parameters k is -33.60538630228388 and b is 77.3534622295036\n",
      "Iteration 694, the loss is 156.33641382079816, parameters k is -33.599101667896534 and b is 77.35446222950361\n",
      "Iteration 695, the loss is 156.29591719141553, parameters k is -33.592817033509185 and b is 77.35546222950362\n",
      "Iteration 696, the loss is 156.25542056203267, parameters k is -33.586532399121836 and b is 77.35646222950362\n",
      "Iteration 697, the loss is 156.21492393265012, parameters k is -33.58024776473449 and b is 77.35746222950362\n",
      "Iteration 698, the loss is 156.1744273032674, parameters k is -33.57396313034714 and b is 77.35846222950363\n",
      "Iteration 699, the loss is 156.1339306738847, parameters k is -33.56767849595979 and b is 77.35946222950363\n",
      "Iteration 700, the loss is 156.0934340445021, parameters k is -33.56139386157244 and b is 77.36046222950364\n",
      "Iteration 701, the loss is 156.05293741511957, parameters k is -33.55510922718509 and b is 77.36146222950364\n",
      "Iteration 702, the loss is 156.01244078573674, parameters k is -33.54882459279774 and b is 77.36246222950365\n",
      "Iteration 703, the loss is 155.97194415635403, parameters k is -33.542539958410394 and b is 77.36346222950365\n",
      "Iteration 704, the loss is 155.93144752697145, parameters k is -33.536255324023045 and b is 77.36446222950366\n",
      "Iteration 705, the loss is 155.8909508975889, parameters k is -33.5299706896357 and b is 77.36546222950366\n",
      "Iteration 706, the loss is 155.85045426820608, parameters k is -33.52368605524835 and b is 77.36646222950367\n",
      "Iteration 707, the loss is 155.80995763882356, parameters k is -33.517401420861 and b is 77.36746222950367\n",
      "Iteration 708, the loss is 155.7694610094407, parameters k is -33.51111678647365 and b is 77.36846222950368\n",
      "Iteration 709, the loss is 155.72896438005796, parameters k is -33.5048321520863 and b is 77.36946222950368\n",
      "Iteration 710, the loss is 155.68846775067536, parameters k is -33.49854751769895 and b is 77.37046222950369\n",
      "Iteration 711, the loss is 155.64797112129273, parameters k is -33.492262883311604 and b is 77.37146222950369\n",
      "Iteration 712, the loss is 155.6074744919102, parameters k is -33.485978248924255 and b is 77.3724622295037\n",
      "Iteration 713, the loss is 155.5669778625274, parameters k is -33.479693614536906 and b is 77.3734622295037\n",
      "Iteration 714, the loss is 155.52648123314466, parameters k is -33.47340898014956 and b is 77.3744622295037\n",
      "Iteration 715, the loss is 155.48598460376198, parameters k is -33.46712434576221 and b is 77.37546222950371\n",
      "Iteration 716, the loss is 155.4454879743795, parameters k is -33.46083971137486 and b is 77.37646222950372\n",
      "Iteration 717, the loss is 155.40499134499686, parameters k is -33.45455507698751 and b is 77.37746222950372\n",
      "Iteration 718, the loss is 155.36449471561394, parameters k is -33.44827044260016 and b is 77.37846222950373\n",
      "Iteration 719, the loss is 155.32399808623148, parameters k is -33.44198580821281 and b is 77.37946222950373\n",
      "Iteration 720, the loss is 155.28350145684863, parameters k is -33.435701173825464 and b is 77.38046222950373\n",
      "Iteration 721, the loss is 155.24300482746597, parameters k is -33.429416539438115 and b is 77.38146222950374\n",
      "Iteration 722, the loss is 155.20250819808325, parameters k is -33.42313190505077 and b is 77.38246222950374\n",
      "Iteration 723, the loss is 155.16201156870065, parameters k is -33.41684727066342 and b is 77.38346222950375\n",
      "Iteration 724, the loss is 155.12151493931805, parameters k is -33.41056263627607 and b is 77.38446222950375\n",
      "Iteration 725, the loss is 155.08101830993544, parameters k is -33.40427800188872 and b is 77.38546222950376\n",
      "Iteration 726, the loss is 155.04052168055287, parameters k is -33.39799336750137 and b is 77.38646222950376\n",
      "Iteration 727, the loss is 155.00002505117013, parameters k is -33.39170873311402 and b is 77.38746222950377\n",
      "Iteration 728, the loss is 154.9595284217875, parameters k is -33.38542409872667 and b is 77.38846222950377\n",
      "Iteration 729, the loss is 154.9190317924047, parameters k is -33.379139464339325 and b is 77.38946222950378\n",
      "Iteration 730, the loss is 154.87853516302195, parameters k is -33.372854829951976 and b is 77.39046222950378\n",
      "Iteration 731, the loss is 154.83803853363918, parameters k is -33.36657019556463 and b is 77.39146222950379\n",
      "Iteration 732, the loss is 154.7975419042566, parameters k is -33.36028556117728 and b is 77.39246222950379\n",
      "Iteration 733, the loss is 154.75704527487395, parameters k is -33.35400092678993 and b is 77.3934622295038\n",
      "Iteration 734, the loss is 154.71654864549117, parameters k is -33.34771629240258 and b is 77.3944622295038\n",
      "Iteration 735, the loss is 154.67605201610868, parameters k is -33.34143165801523 and b is 77.3954622295038\n",
      "Iteration 736, the loss is 154.63555538672605, parameters k is -33.33514702362788 and b is 77.39646222950381\n",
      "Iteration 737, the loss is 154.59505875734348, parameters k is -33.328862389240534 and b is 77.39746222950382\n",
      "Iteration 738, the loss is 154.55456212796045, parameters k is -33.322577754853185 and b is 77.39846222950382\n",
      "Iteration 739, the loss is 154.514065498578, parameters k is -33.316293120465836 and b is 77.39946222950383\n",
      "Iteration 740, the loss is 154.47356886919533, parameters k is -33.31000848607849 and b is 77.40046222950383\n",
      "Iteration 741, the loss is 154.4330722398124, parameters k is -33.30372385169114 and b is 77.40146222950384\n",
      "Iteration 742, the loss is 154.3925756104301, parameters k is -33.29743921730379 and b is 77.40246222950384\n",
      "Iteration 743, the loss is 154.35207898104727, parameters k is -33.29115458291644 and b is 77.40346222950384\n",
      "Iteration 744, the loss is 154.31158235166453, parameters k is -33.28486994852909 and b is 77.40446222950385\n",
      "Iteration 745, the loss is 154.27108572228195, parameters k is -33.27858531414174 and b is 77.40546222950385\n",
      "Iteration 746, the loss is 154.2305890928993, parameters k is -33.272300679754395 and b is 77.40646222950386\n",
      "Iteration 747, the loss is 154.19009246351658, parameters k is -33.266016045367046 and b is 77.40746222950386\n",
      "Iteration 748, the loss is 154.14959583413378, parameters k is -33.2597314109797 and b is 77.40846222950387\n",
      "Iteration 749, the loss is 154.10909920475115, parameters k is -33.25344677659235 and b is 77.40946222950387\n",
      "Iteration 750, the loss is 154.0686025753686, parameters k is -33.247162142205 and b is 77.41046222950388\n",
      "Iteration 751, the loss is 154.02810594598594, parameters k is -33.24087750781765 and b is 77.41146222950388\n",
      "Iteration 752, the loss is 153.98760931660317, parameters k is -33.2345928734303 and b is 77.41246222950389\n",
      "Iteration 753, the loss is 153.9471126872205, parameters k is -33.22830823904295 and b is 77.41346222950389\n",
      "Iteration 754, the loss is 153.90661605783797, parameters k is -33.222023604655604 and b is 77.4144622295039\n",
      "Iteration 755, the loss is 153.86611942845514, parameters k is -33.215738970268255 and b is 77.4154622295039\n",
      "Iteration 756, the loss is 153.8256227990727, parameters k is -33.209454335880906 and b is 77.4164622295039\n",
      "Iteration 757, the loss is 153.7851261696899, parameters k is -33.20316970149356 and b is 77.41746222950391\n",
      "Iteration 758, the loss is 153.74462954030733, parameters k is -33.19688506710621 and b is 77.41846222950392\n",
      "Iteration 759, the loss is 153.7041329109246, parameters k is -33.19060043271886 and b is 77.41946222950392\n",
      "Iteration 760, the loss is 153.6636362815418, parameters k is -33.18431579833151 and b is 77.42046222950393\n",
      "Iteration 761, the loss is 153.6231396521592, parameters k is -33.17803116394416 and b is 77.42146222950393\n",
      "Iteration 762, the loss is 153.58264302277664, parameters k is -33.17174652955681 and b is 77.42246222950394\n",
      "Iteration 763, the loss is 153.54214639339398, parameters k is -33.165461895169464 and b is 77.42346222950394\n",
      "Iteration 764, the loss is 153.5016497640113, parameters k is -33.159177260782116 and b is 77.42446222950394\n",
      "Iteration 765, the loss is 153.4611531346285, parameters k is -33.15289262639477 and b is 77.42546222950395\n",
      "Iteration 766, the loss is 153.42065650524592, parameters k is -33.14660799200742 and b is 77.42646222950395\n",
      "Iteration 767, the loss is 153.38015987586314, parameters k is -33.14032335762007 and b is 77.42746222950396\n",
      "Iteration 768, the loss is 153.33966324648043, parameters k is -33.13403872323272 and b is 77.42846222950396\n",
      "Iteration 769, the loss is 153.29916661709782, parameters k is -33.12775408884537 and b is 77.42946222950397\n",
      "Iteration 770, the loss is 153.25866998771508, parameters k is -33.12146945445802 and b is 77.43046222950397\n",
      "Iteration 771, the loss is 153.21817335833236, parameters k is -33.115184820070674 and b is 77.43146222950398\n",
      "Iteration 772, the loss is 153.17767672894993, parameters k is -33.108900185683325 and b is 77.43246222950398\n",
      "Iteration 773, the loss is 153.1371800995672, parameters k is -33.102615551295976 and b is 77.43346222950399\n",
      "Iteration 774, the loss is 153.0966834701847, parameters k is -33.09633091690863 and b is 77.43446222950399\n",
      "Iteration 775, the loss is 153.0561868408017, parameters k is -33.09004628252128 and b is 77.435462229504\n",
      "Iteration 776, the loss is 153.0156902114191, parameters k is -33.08376164813393 and b is 77.436462229504\n",
      "Iteration 777, the loss is 152.97519358203638, parameters k is -33.07747701374658 and b is 77.437462229504\n",
      "Iteration 778, the loss is 152.93469695265387, parameters k is -33.07119237935923 and b is 77.43846222950401\n",
      "Iteration 779, the loss is 152.89420032327124, parameters k is -33.06490774497188 and b is 77.43946222950402\n",
      "Iteration 780, the loss is 152.85370369388832, parameters k is -33.058623110584534 and b is 77.44046222950402\n",
      "Iteration 781, the loss is 152.81320706450583, parameters k is -33.052338476197185 and b is 77.44146222950403\n",
      "Iteration 782, the loss is 152.77271043512295, parameters k is -33.04605384180984 and b is 77.44246222950403\n",
      "Iteration 783, the loss is 152.7322138057405, parameters k is -33.03976920742249 and b is 77.44346222950404\n",
      "Iteration 784, the loss is 152.69171717635777, parameters k is -33.03348457303514 and b is 77.44446222950404\n",
      "Iteration 785, the loss is 152.65122054697497, parameters k is -33.02719993864779 and b is 77.44546222950405\n",
      "Iteration 786, the loss is 152.61072391759262, parameters k is -33.02091530426044 and b is 77.44646222950405\n",
      "Iteration 787, the loss is 152.57022728820982, parameters k is -33.01463066987309 and b is 77.44746222950405\n",
      "Iteration 788, the loss is 152.5297306588269, parameters k is -33.008346035485744 and b is 77.44846222950406\n",
      "Iteration 789, the loss is 152.48923402944442, parameters k is -33.002061401098395 and b is 77.44946222950406\n",
      "Iteration 790, the loss is 152.44873740006184, parameters k is -32.995776766711046 and b is 77.45046222950407\n",
      "Iteration 791, the loss is 152.4082407706791, parameters k is -32.9894921323237 and b is 77.45146222950407\n",
      "Iteration 792, the loss is 152.36774414129624, parameters k is -32.98320749793635 and b is 77.45246222950408\n",
      "Iteration 793, the loss is 152.32724751191375, parameters k is -32.976922863549 and b is 77.45346222950408\n",
      "Iteration 794, the loss is 152.286750882531, parameters k is -32.97063822916165 and b is 77.45446222950409\n",
      "Iteration 795, the loss is 152.2462542531483, parameters k is -32.9643535947743 and b is 77.45546222950409\n",
      "Iteration 796, the loss is 152.20575762376578, parameters k is -32.95806896038695 and b is 77.4564622295041\n",
      "Iteration 797, the loss is 152.16526099438312, parameters k is -32.951784325999604 and b is 77.4574622295041\n",
      "Iteration 798, the loss is 152.12476436500046, parameters k is -32.945499691612255 and b is 77.45846222950411\n",
      "Iteration 799, the loss is 152.08426773561752, parameters k is -32.93921505722491 and b is 77.45946222950411\n",
      "Iteration 800, the loss is 152.04377110623523, parameters k is -32.93293042283756 and b is 77.46046222950412\n",
      "Iteration 801, the loss is 152.00327447685234, parameters k is -32.92664578845021 and b is 77.46146222950412\n",
      "Iteration 802, the loss is 151.9627778474695, parameters k is -32.92036115406286 and b is 77.46246222950413\n",
      "Iteration 803, the loss is 151.92228121808702, parameters k is -32.91407651967551 and b is 77.46346222950413\n",
      "Iteration 804, the loss is 151.88178458870428, parameters k is -32.90779188528816 and b is 77.46446222950414\n",
      "Iteration 805, the loss is 151.84128795932173, parameters k is -32.90150725090081 and b is 77.46546222950414\n",
      "Iteration 806, the loss is 151.80079132993913, parameters k is -32.895222616513465 and b is 77.46646222950415\n",
      "Iteration 807, the loss is 151.76029470055644, parameters k is -32.888937982126116 and b is 77.46746222950415\n",
      "Iteration 808, the loss is 151.71979807117347, parameters k is -32.88265334773877 and b is 77.46846222950415\n",
      "Iteration 809, the loss is 151.6793014417909, parameters k is -32.87636871335142 and b is 77.46946222950416\n",
      "Iteration 810, the loss is 151.6388048124083, parameters k is -32.87008407896407 and b is 77.47046222950416\n",
      "Iteration 811, the loss is 151.5983081830257, parameters k is -32.86379944457672 and b is 77.47146222950417\n",
      "Iteration 812, the loss is 151.55781155364318, parameters k is -32.85751481018937 and b is 77.47246222950417\n",
      "Iteration 813, the loss is 151.51731492426032, parameters k is -32.85123017580202 and b is 77.47346222950418\n",
      "Iteration 814, the loss is 151.47681829487775, parameters k is -32.844945541414674 and b is 77.47446222950418\n",
      "Iteration 815, the loss is 151.436321665495, parameters k is -32.838660907027325 and b is 77.47546222950419\n",
      "Iteration 816, the loss is 151.39582503611243, parameters k is -32.832376272639976 and b is 77.4764622295042\n",
      "Iteration 817, the loss is 151.35532840672968, parameters k is -32.82609163825263 and b is 77.4774622295042\n",
      "Iteration 818, the loss is 151.3148317773469, parameters k is -32.81980700386528 and b is 77.4784622295042\n",
      "Iteration 819, the loss is 151.27433514796425, parameters k is -32.81352236947793 and b is 77.47946222950421\n",
      "Iteration 820, the loss is 151.23383851858168, parameters k is -32.80723773509058 and b is 77.48046222950421\n",
      "Iteration 821, the loss is 151.19334188919888, parameters k is -32.80095310070323 and b is 77.48146222950422\n",
      "Iteration 822, the loss is 151.15284525981616, parameters k is -32.79466846631588 and b is 77.48246222950422\n",
      "Iteration 823, the loss is 151.1123486304336, parameters k is -32.788383831928535 and b is 77.48346222950423\n",
      "Iteration 824, the loss is 151.07185200105081, parameters k is -32.782099197541186 and b is 77.48446222950423\n",
      "Iteration 825, the loss is 151.0313553716683, parameters k is -32.77581456315384 and b is 77.48546222950424\n",
      "Iteration 826, the loss is 150.99085874228555, parameters k is -32.76952992876649 and b is 77.48646222950424\n",
      "Iteration 827, the loss is 150.9503621129031, parameters k is -32.76324529437914 and b is 77.48746222950425\n",
      "Iteration 828, the loss is 150.9098654835204, parameters k is -32.75696065999179 and b is 77.48846222950425\n",
      "Iteration 829, the loss is 150.8693688541375, parameters k is -32.75067602560444 and b is 77.48946222950426\n",
      "Iteration 830, the loss is 150.82887222475506, parameters k is -32.74439139121709 and b is 77.49046222950426\n",
      "Iteration 831, the loss is 150.78837559537223, parameters k is -32.738106756829744 and b is 77.49146222950426\n",
      "Iteration 832, the loss is 150.74787896598932, parameters k is -32.731822122442395 and b is 77.49246222950427\n",
      "Iteration 833, the loss is 150.707382336607, parameters k is -32.725537488055046 and b is 77.49346222950427\n",
      "Iteration 834, the loss is 150.66688570722422, parameters k is -32.7192528536677 and b is 77.49446222950428\n",
      "Iteration 835, the loss is 150.6263890778416, parameters k is -32.71296821928035 and b is 77.49546222950428\n",
      "Iteration 836, the loss is 150.58589244845885, parameters k is -32.706683584893 and b is 77.49646222950429\n",
      "Iteration 837, the loss is 150.54539581907622, parameters k is -32.70039895050565 and b is 77.4974622295043\n",
      "Iteration 838, the loss is 150.5048991896935, parameters k is -32.6941143161183 and b is 77.4984622295043\n",
      "Iteration 839, the loss is 150.46440256031082, parameters k is -32.68782968173095 and b is 77.4994622295043\n",
      "Iteration 840, the loss is 150.42390593092816, parameters k is -32.681545047343604 and b is 77.50046222950431\n",
      "Iteration 841, the loss is 150.38340930154556, parameters k is -32.675260412956256 and b is 77.50146222950431\n",
      "Iteration 842, the loss is 150.34291267216278, parameters k is -32.66897577856891 and b is 77.50246222950432\n",
      "Iteration 843, the loss is 150.30241604278018, parameters k is -32.66269114418156 and b is 77.50346222950432\n",
      "Iteration 844, the loss is 150.2619194133976, parameters k is -32.65640650979421 and b is 77.50446222950433\n",
      "Iteration 845, the loss is 150.22142278401472, parameters k is -32.65012187540686 and b is 77.50546222950433\n",
      "Iteration 846, the loss is 150.18092615463217, parameters k is -32.64383724101951 and b is 77.50646222950434\n",
      "Iteration 847, the loss is 150.14042952524966, parameters k is -32.63755260663216 and b is 77.50746222950434\n",
      "Iteration 848, the loss is 150.09993289586689, parameters k is -32.631267972244814 and b is 77.50846222950435\n",
      "Iteration 849, the loss is 150.05943626648425, parameters k is -32.624983337857465 and b is 77.50946222950435\n",
      "Iteration 850, the loss is 150.01893963710137, parameters k is -32.618698703470116 and b is 77.51046222950436\n",
      "Iteration 851, the loss is 149.97844300771882, parameters k is -32.61241406908277 and b is 77.51146222950436\n",
      "Iteration 852, the loss is 149.9379463783361, parameters k is -32.60612943469542 and b is 77.51246222950437\n",
      "Iteration 853, the loss is 149.89744974895342, parameters k is -32.59984480030807 and b is 77.51346222950437\n",
      "Iteration 854, the loss is 149.85695311957082, parameters k is -32.59356016592072 and b is 77.51446222950437\n",
      "Iteration 855, the loss is 149.81645649018802, parameters k is -32.58727553153337 and b is 77.51546222950438\n",
      "Iteration 856, the loss is 149.77595986080544, parameters k is -32.58099089714602 and b is 77.51646222950438\n",
      "Iteration 857, the loss is 149.73546323142287, parameters k is -32.574706262758674 and b is 77.51746222950439\n",
      "Iteration 858, the loss is 149.69496660204018, parameters k is -32.568421628371325 and b is 77.5184622295044\n",
      "Iteration 859, the loss is 149.65446997265732, parameters k is -32.56213699398398 and b is 77.5194622295044\n",
      "Iteration 860, the loss is 149.61397334327478, parameters k is -32.55585235959663 and b is 77.5204622295044\n",
      "Iteration 861, the loss is 149.57347671389218, parameters k is -32.54956772520928 and b is 77.52146222950441\n",
      "Iteration 862, the loss is 149.53298008450952, parameters k is -32.54328309082193 and b is 77.52246222950441\n",
      "Iteration 863, the loss is 149.4924834551267, parameters k is -32.53699845643458 and b is 77.52346222950442\n",
      "Iteration 864, the loss is 149.45198682574403, parameters k is -32.53071382204723 and b is 77.52446222950442\n",
      "Iteration 865, the loss is 149.41149019636148, parameters k is -32.524429187659884 and b is 77.52546222950443\n",
      "Iteration 866, the loss is 149.37099356697877, parameters k is -32.518144553272535 and b is 77.52646222950443\n",
      "Iteration 867, the loss is 149.3304969375962, parameters k is -32.511859918885186 and b is 77.52746222950444\n",
      "Iteration 868, the loss is 149.2900003082136, parameters k is -32.50557528449784 and b is 77.52846222950444\n",
      "Iteration 869, the loss is 149.24950367883068, parameters k is -32.49929065011049 and b is 77.52946222950445\n",
      "Iteration 870, the loss is 149.20900704944808, parameters k is -32.49300601572314 and b is 77.53046222950445\n",
      "Iteration 871, the loss is 149.16851042006547, parameters k is -32.48672138133579 and b is 77.53146222950446\n",
      "Iteration 872, the loss is 149.12801379068284, parameters k is -32.48043674694844 and b is 77.53246222950446\n",
      "Iteration 873, the loss is 149.08751716129998, parameters k is -32.47415211256109 and b is 77.53346222950447\n",
      "Iteration 874, the loss is 149.04702053191735, parameters k is -32.467867478173744 and b is 77.53446222950447\n",
      "Iteration 875, the loss is 149.00652390253478, parameters k is -32.461582843786395 and b is 77.53546222950447\n",
      "Iteration 876, the loss is 148.96602727315184, parameters k is -32.45529820939905 and b is 77.53646222950448\n",
      "Iteration 877, the loss is 148.92553064376946, parameters k is -32.4490135750117 and b is 77.53746222950448\n",
      "Iteration 878, the loss is 148.8850340143868, parameters k is -32.44272894062435 and b is 77.53846222950449\n",
      "Iteration 879, the loss is 148.84453738500403, parameters k is -32.436444306237 and b is 77.5394622295045\n",
      "Iteration 880, the loss is 148.80404075562117, parameters k is -32.43015967184965 and b is 77.5404622295045\n",
      "Iteration 881, the loss is 148.76354412623888, parameters k is -32.4238750374623 and b is 77.5414622295045\n",
      "Iteration 882, the loss is 148.72304749685614, parameters k is -32.41759040307495 and b is 77.54246222950451\n",
      "Iteration 883, the loss is 148.68255086747337, parameters k is -32.411305768687605 and b is 77.54346222950451\n",
      "Iteration 884, the loss is 148.64205423809062, parameters k is -32.405021134300256 and b is 77.54446222950452\n",
      "Iteration 885, the loss is 148.60155760870802, parameters k is -32.39873649991291 and b is 77.54546222950452\n",
      "Iteration 886, the loss is 148.56106097932516, parameters k is -32.39245186552556 and b is 77.54646222950453\n",
      "Iteration 887, the loss is 148.52056434994253, parameters k is -32.38616723113821 and b is 77.54746222950453\n",
      "Iteration 888, the loss is 148.48006772056016, parameters k is -32.37988259675086 and b is 77.54846222950454\n",
      "Iteration 889, the loss is 148.43957109117738, parameters k is -32.37359796236351 and b is 77.54946222950454\n",
      "Iteration 890, the loss is 148.39907446179473, parameters k is -32.36731332797616 and b is 77.55046222950455\n",
      "Iteration 891, the loss is 148.35857783241184, parameters k is -32.361028693588814 and b is 77.55146222950455\n",
      "Iteration 892, the loss is 148.31808120302924, parameters k is -32.354744059201465 and b is 77.55246222950456\n",
      "Iteration 893, the loss is 148.27758457364666, parameters k is -32.348459424814116 and b is 77.55346222950456\n",
      "Iteration 894, the loss is 148.23708794426415, parameters k is -32.34217479042677 and b is 77.55446222950457\n",
      "Iteration 895, the loss is 148.19659131488132, parameters k is -32.33589015603942 and b is 77.55546222950457\n",
      "Iteration 896, the loss is 148.1560946854985, parameters k is -32.32960552165207 and b is 77.55646222950458\n",
      "Iteration 897, the loss is 148.11559805611577, parameters k is -32.32332088726472 and b is 77.55746222950458\n",
      "Iteration 898, the loss is 148.0751014267332, parameters k is -32.31703625287737 and b is 77.55846222950458\n",
      "Iteration 899, the loss is 148.0346047973507, parameters k is -32.31075161849002 and b is 77.55946222950459\n",
      "Iteration 900, the loss is 147.99410816796785, parameters k is -32.304466984102675 and b is 77.5604622295046\n",
      "Iteration 901, the loss is 147.95361153858525, parameters k is -32.298182349715326 and b is 77.5614622295046\n",
      "Iteration 902, the loss is 147.91311490920253, parameters k is -32.29189771532798 and b is 77.5624622295046\n",
      "Iteration 903, the loss is 147.87261827982007, parameters k is -32.28561308094063 and b is 77.56346222950461\n",
      "Iteration 904, the loss is 147.8321216504374, parameters k is -32.27932844655328 and b is 77.56446222950461\n",
      "Iteration 905, the loss is 147.79162502105459, parameters k is -32.27304381216593 and b is 77.56546222950462\n",
      "Iteration 906, the loss is 147.75112839167176, parameters k is -32.26675917777858 and b is 77.56646222950462\n",
      "Iteration 907, the loss is 147.71063176228938, parameters k is -32.26047454339123 and b is 77.56746222950463\n",
      "Iteration 908, the loss is 147.67013513290644, parameters k is -32.254189909003884 and b is 77.56846222950463\n",
      "Iteration 909, the loss is 147.6296385035238, parameters k is -32.247905274616535 and b is 77.56946222950464\n",
      "Iteration 910, the loss is 147.58914187414135, parameters k is -32.241620640229186 and b is 77.57046222950464\n",
      "Iteration 911, the loss is 147.54864524475866, parameters k is -32.23533600584184 and b is 77.57146222950465\n",
      "Iteration 912, the loss is 147.508148615376, parameters k is -32.22905137145449 and b is 77.57246222950465\n",
      "Iteration 913, the loss is 147.4676519859932, parameters k is -32.22276673706714 and b is 77.57346222950466\n",
      "Iteration 914, the loss is 147.4271553566104, parameters k is -32.21648210267979 and b is 77.57446222950466\n",
      "Iteration 915, the loss is 147.38665872722794, parameters k is -32.21019746829244 and b is 77.57546222950467\n",
      "Iteration 916, the loss is 147.3461620978453, parameters k is -32.20391283390509 and b is 77.57646222950467\n",
      "Iteration 917, the loss is 147.30566546846265, parameters k is -32.197628199517744 and b is 77.57746222950468\n",
      "Iteration 918, the loss is 147.2651688390799, parameters k is -32.191343565130396 and b is 77.57846222950468\n",
      "Iteration 919, the loss is 147.2246722096973, parameters k is -32.18505893074305 and b is 77.57946222950468\n",
      "Iteration 920, the loss is 147.18417558031453, parameters k is -32.1787742963557 and b is 77.58046222950469\n",
      "Iteration 921, the loss is 147.14367895093181, parameters k is -32.17248966196835 and b is 77.5814622295047\n",
      "Iteration 922, the loss is 147.10318232154904, parameters k is -32.166205027581 and b is 77.5824622295047\n",
      "Iteration 923, the loss is 147.06268569216658, parameters k is -32.15992039319365 and b is 77.5834622295047\n",
      "Iteration 924, the loss is 147.02218906278387, parameters k is -32.1536357588063 and b is 77.58446222950471\n",
      "Iteration 925, the loss is 146.98169243340112, parameters k is -32.147351124418954 and b is 77.58546222950471\n",
      "Iteration 926, the loss is 146.94119580401863, parameters k is -32.141066490031605 and b is 77.58646222950472\n",
      "Iteration 927, the loss is 146.90069917463575, parameters k is -32.134781855644256 and b is 77.58746222950472\n",
      "Iteration 928, the loss is 146.8602025452532, parameters k is -32.12849722125691 and b is 77.58846222950473\n",
      "Iteration 929, the loss is 146.81970591587057, parameters k is -32.12221258686956 and b is 77.58946222950473\n",
      "Iteration 930, the loss is 146.7792092864878, parameters k is -32.11592795248221 and b is 77.59046222950474\n",
      "Iteration 931, the loss is 146.7387126571051, parameters k is -32.10964331809486 and b is 77.59146222950474\n",
      "Iteration 932, the loss is 146.69821602772254, parameters k is -32.10335868370751 and b is 77.59246222950475\n",
      "Iteration 933, the loss is 146.65771939833988, parameters k is -32.09707404932016 and b is 77.59346222950475\n",
      "Iteration 934, the loss is 146.61722276895722, parameters k is -32.090789414932814 and b is 77.59446222950476\n",
      "Iteration 935, the loss is 146.5767261395744, parameters k is -32.084504780545466 and b is 77.59546222950476\n",
      "Iteration 936, the loss is 146.53622951019176, parameters k is -32.07822014615812 and b is 77.59646222950477\n",
      "Iteration 937, the loss is 146.495732880809, parameters k is -32.07193551177077 and b is 77.59746222950477\n",
      "Iteration 938, the loss is 146.45523625142638, parameters k is -32.06565087738342 and b is 77.59846222950478\n",
      "Iteration 939, the loss is 146.4147396220437, parameters k is -32.05936624299607 and b is 77.59946222950478\n",
      "Iteration 940, the loss is 146.3742429926611, parameters k is -32.05308160860872 and b is 77.60046222950479\n",
      "Iteration 941, the loss is 146.33374636327832, parameters k is -32.04679697422137 and b is 77.60146222950479\n",
      "Iteration 942, the loss is 146.29324973389578, parameters k is -32.040512339834024 and b is 77.6024622295048\n",
      "Iteration 943, the loss is 146.25275310451315, parameters k is -32.034227705446675 and b is 77.6034622295048\n",
      "Iteration 944, the loss is 146.21225647513052, parameters k is -32.027943071059326 and b is 77.6044622295048\n",
      "Iteration 945, the loss is 146.17175984574763, parameters k is -32.02165843667198 and b is 77.60546222950481\n",
      "Iteration 946, the loss is 146.13126321636494, parameters k is -32.01537380228463 and b is 77.60646222950481\n",
      "Iteration 947, the loss is 146.09076658698243, parameters k is -32.00908916789728 and b is 77.60746222950482\n",
      "Iteration 948, the loss is 146.0502699575998, parameters k is -32.00280453350993 and b is 77.60846222950482\n",
      "Iteration 949, the loss is 146.009773328217, parameters k is -31.99651989912258 and b is 77.60946222950483\n",
      "Iteration 950, the loss is 145.96927669883442, parameters k is -31.990235264735226 and b is 77.61046222950483\n",
      "Iteration 951, the loss is 145.9287800694518, parameters k is -31.983950630347874 and b is 77.61146222950484\n",
      "Iteration 952, the loss is 145.88828344006893, parameters k is -31.97766599596052 and b is 77.61246222950484\n",
      "Iteration 953, the loss is 145.84778681068624, parameters k is -31.97138136157317 and b is 77.61346222950485\n",
      "Iteration 954, the loss is 145.8072901813036, parameters k is -31.965096727185816 and b is 77.61446222950485\n",
      "Iteration 955, the loss is 145.76679355192078, parameters k is -31.958812092798464 and b is 77.61546222950486\n",
      "Iteration 956, the loss is 145.72629692253824, parameters k is -31.95252745841111 and b is 77.61646222950486\n",
      "Iteration 957, the loss is 145.68580029315558, parameters k is -31.94624282402376 and b is 77.61746222950487\n",
      "Iteration 958, the loss is 145.6453036637727, parameters k is -31.939958189636407 and b is 77.61846222950487\n",
      "Iteration 959, the loss is 145.60480703439003, parameters k is -31.933673555249054 and b is 77.61946222950488\n",
      "Iteration 960, the loss is 145.56431040500752, parameters k is -31.927388920861702 and b is 77.62046222950488\n",
      "Iteration 961, the loss is 145.52381377562483, parameters k is -31.92110428647435 and b is 77.62146222950489\n",
      "Iteration 962, the loss is 145.48331714624197, parameters k is -31.914819652086997 and b is 77.62246222950489\n",
      "Iteration 963, the loss is 145.4428205168594, parameters k is -31.908535017699645 and b is 77.6234622295049\n",
      "Iteration 964, the loss is 145.40232388747663, parameters k is -31.902250383312293 and b is 77.6244622295049\n",
      "Iteration 965, the loss is 145.36182725809402, parameters k is -31.89596574892494 and b is 77.6254622295049\n",
      "Iteration 966, the loss is 145.32133062871125, parameters k is -31.889681114537588 and b is 77.62646222950491\n",
      "Iteration 967, the loss is 145.2808339993285, parameters k is -31.883396480150235 and b is 77.62746222950491\n",
      "Iteration 968, the loss is 145.24033736994565, parameters k is -31.877111845762883 and b is 77.62846222950492\n",
      "Iteration 969, the loss is 145.19984074056308, parameters k is -31.87082721137553 and b is 77.62946222950492\n",
      "Iteration 970, the loss is 145.15934411118042, parameters k is -31.86454257698818 and b is 77.63046222950493\n",
      "Iteration 971, the loss is 145.11884748179776, parameters k is -31.858257942600826 and b is 77.63146222950493\n",
      "Iteration 972, the loss is 145.07835085241516, parameters k is -31.851973308213474 and b is 77.63246222950494\n",
      "Iteration 973, the loss is 145.0378542230323, parameters k is -31.84568867382612 and b is 77.63346222950494\n",
      "Iteration 974, the loss is 144.99735759364964, parameters k is -31.83940403943877 and b is 77.63446222950495\n",
      "Iteration 975, the loss is 144.95686096426707, parameters k is -31.833119405051416 and b is 77.63546222950495\n",
      "Iteration 976, the loss is 144.9163643348844, parameters k is -31.826834770664064 and b is 77.63646222950496\n",
      "Iteration 977, the loss is 144.87586770550172, parameters k is -31.82055013627671 and b is 77.63746222950496\n",
      "Iteration 978, the loss is 144.83537107611892, parameters k is -31.81426550188936 and b is 77.63846222950497\n",
      "Iteration 979, the loss is 144.79487444673617, parameters k is -31.807980867502007 and b is 77.63946222950497\n",
      "Iteration 980, the loss is 144.7543778173535, parameters k is -31.801696233114654 and b is 77.64046222950498\n",
      "Iteration 981, the loss is 144.71388118797077, parameters k is -31.795411598727302 and b is 77.64146222950498\n",
      "Iteration 982, the loss is 144.6733845585884, parameters k is -31.78912696433995 and b is 77.64246222950499\n",
      "Iteration 983, the loss is 144.63288792920554, parameters k is -31.782842329952597 and b is 77.64346222950499\n",
      "Iteration 984, the loss is 144.5923912998228, parameters k is -31.776557695565245 and b is 77.644462229505\n",
      "Iteration 985, the loss is 144.55189467044008, parameters k is -31.770273061177893 and b is 77.645462229505\n",
      "Iteration 986, the loss is 144.5113980410575, parameters k is -31.76398842679054 and b is 77.646462229505\n",
      "Iteration 987, the loss is 144.47090141167467, parameters k is -31.757703792403188 and b is 77.64746222950501\n",
      "Iteration 988, the loss is 144.43040478229213, parameters k is -31.751419158015835 and b is 77.64846222950501\n",
      "Iteration 989, the loss is 144.38990815290944, parameters k is -31.745134523628483 and b is 77.64946222950502\n",
      "Iteration 990, the loss is 144.34941152352678, parameters k is -31.73884988924113 and b is 77.65046222950502\n",
      "Iteration 991, the loss is 144.30891489414398, parameters k is -31.732565254853778 and b is 77.65146222950503\n",
      "Iteration 992, the loss is 144.26841826476138, parameters k is -31.726280620466426 and b is 77.65246222950503\n",
      "Iteration 993, the loss is 144.22792163537866, parameters k is -31.719995986079073 and b is 77.65346222950504\n",
      "Iteration 994, the loss is 144.18742500599592, parameters k is -31.71371135169172 and b is 77.65446222950504\n",
      "Iteration 995, the loss is 144.14692837661312, parameters k is -31.70742671730437 and b is 77.65546222950505\n",
      "Iteration 996, the loss is 144.10643174723052, parameters k is -31.701142082917016 and b is 77.65646222950505\n",
      "Iteration 997, the loss is 144.06593511784766, parameters k is -31.694857448529664 and b is 77.65746222950506\n",
      "Iteration 998, the loss is 144.025438488465, parameters k is -31.68857281414231 and b is 77.65846222950506\n",
      "Iteration 999, the loss is 143.98494185908245, parameters k is -31.68228817975496 and b is 77.65946222950507\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k = random.random() * 200 - 100  # -100 100\n",
    "b = random.random() * 200 - 100  # -100 100\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "iteration_num = 1000 \n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    \n",
    "    current_loss = loss(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = partial_derivative_k(X_rm)\n",
    "    b_gradient = partial_derivative_b()\n",
    "    \n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b\n",
    "# 用mae并不总能收敛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
